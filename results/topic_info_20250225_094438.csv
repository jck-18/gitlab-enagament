,Topic,Count,Name,Representation,Representative_Docs
0,-1,27,-1_pipeline_gitlab_build_test,"['pipeline', 'gitlab', 'build', 'test', 'pipelines', 'https', 'run', 'job', 'docker', 'rules']","['I\'m hoping someone can help me with this one.\n\nGitlab version: v17.6.1-ee\n\nI have a \\`.gitlab-ci.yml\\` file. It has the following basic config derived from the [GL docs](https://docs.gitlab.com/ee/ci/pipelines/merge_request_pipelines.html:)\n\n    workflow:\n      rules:\n        - if: $CI_PIPELINE_SOURCE == ""merge_request_event""\n    \n    default:\n      image: node:lts\n      tags:\n        - eks\n    \n    stages:\n      - install\n    \n    install:\n      stage: install\n      script:\n        - env\n      rules:\n        - if: $CI_PIPELINE_SOURCE == \'merge_request_event\'\n\nThe problem presents itself when you have a pending merge request opened, and a test branch created.\n\nIf I create a commit with the message of ""test"" - my pipeline runs and works as expected.\n\nIf I create a commit with the message of ""fix: test"" - my pipeline does not run. Instead, GitLab thinks the commit is a branch pipeline, which my workflow rules deny.\n\nDoes anyone know why is the behavior changing from a merge request pipeline to a branch pipeline based on the "":"" in the commit message?\n\n  \nEdit:\n\nThe crux of the issue was our GitLab server - a restart of the service/stack magically fixed the problem and my CI is now functioning as anticipated again when pushing commits with a colon in the message.', 'Hi, all!\n\nI have an issue with a pipeline that I just cannot resolve - probably just a big skill issue.\n\nMy project has a scheduled pipeline for build & internal env deploy twice a week. I don\'t want the project to build continuously due to the nature of our work.\n\nEvery job in a pipeline has a following rule to achieve this:\n\n    rules:  \n    - if: \'$CI_PIPELINE_SOURCE == ""schedule""\'\n\nThere was a request from the dev team that there should be a possilibity to have a pipeline started manually which would only trigger build and test jobs and not the deployment job.\n\nSo I\'ve modified the rules of my build & test jobs to the following:\n\n    - if: \'$CI_PIPELINE_SOURCE == ""schedule"" || $CI_PIPELINE_SOURCE == ""manual""\'\n\nThe deployment job retained the rule it first had (to only execute on schedule).\n\nHowever. When I now want to execute the pipeline manually, I get the following error:\n\n    Pipeline cannot be run.  \n    The resulting pipeline would have been empty. Review the rules configuration for the relevant jobs.\n\nI\'ve first also tried to make rules of build & test jobs as following:\n\n    rules:  \n      - if: \'$CI_PIPELINE_SOURCE == ""schedule""\'  \n      - if: \'$CI_PIPELINE_SOURCE == ""manual""\'\n\nBut also no success. It\'s just as the second rule doesn\'t get evaluated for some reason...\n\nAny ideas? Thanks!', ""We currently have an issue that allows users to merge code that fails tests. I have read the docs and didn't find any useful feature, and googling also didn't lead me to any solution (but tbh I'm not 100% sure what keywords to search for). I was so desperate that I asked ChatGPT, and this also didn't give me anything that would fulfill our requirements.\n\nWe have a bunch of resource-intensive tests in our backend repository. These tests are skipped when the last commit has no changes to the code that's being tested (`rules:changes` keyword without any reference), or when a pipeline is run as a downstream pipeline from the frontend repository.\n\nWe specifically want to avoid running these tests when they are not necessary, like when there are changes only to the frontend, or to the documentation, or similar.\n\nMerge requests are configured to only allow merging when the pipeline has succeeded.\n\nHowever, the following sequence of events can lead to a user being able to merge even when the test jobs have failed:\n\n* create merge request, work on backend code, last pipeline failed in the test job\n* push a commit which creates a pipeline that does not start the test (or push to the frontend and run a downstream pipeline here), pipeline succeeds\n* user is allowed to merge\n\nMy best idea currently is to write a job which checks the state of each test job in all past pipelines of the branch, and fails if the last run instance of the job has failed. But this feels pretty hacky, and would also mean that upstream pipelines would be marked as failed.\n\nSure, we could raise awareness for devs, but the reality is they sometimes just don't think about it or aren't aware that there even is a failed pipeline in the past. Just requiring the last pipeline to have not skipped the tests before merging would also be a solution.\n\nDoes anyone know any feature that could help us? Is there even any way to prevent this from happening? For example: consider a skipped job failed when it failed during the last pipeline, or consider the pipeline failed when there is any job that hasn't been run since it last failed for the branch.""]"
1,0,44,0_gitlab_2025_20_com,"['gitlab', '2025', '20', 'com', '11', '57', 'https', 'gitlab com', 'docker', 'https gitlab']","[""I try to create backups of my self-hosted GitLab (running in Docker container) with `sudo docker exec -t gitlab gitlab-backup create`. However, when i check `backups` the directory is still empty. Im grateful about any ideas what i'm missing!\n\n\n\nThis is the output:\n\n    2025-01-13 20:11:52 UTC -- Dumping database ...\n    2025-01-13 20:11:52 UTC -- Dumping PostgreSQL database gitlabhq_production ...\n    2025-01-13 20:11:56 UTC -- [DONE]\n    2025-01-13 20:11:56 UTC -- Dumping database ... done\n    2025-01-13 20:11:56 UTC -- Dumping repositories ...\n    ...\n    2025-01-13 20:11:57 UTC -- Dumping repositories ... done\n    2025-01-13 20:11:57 UTC -- Dumping uploads ...\n    2025-01-13 20:11:57 UTC -- Dumping uploads ... done\n    2025-01-13 20:11:57 UTC -- Dumping builds ...\n    2025-01-13 20:11:57 UTC -- Dumping builds ... done\n    2025-01-13 20:11:57 UTC -- Dumping artifacts ...\n    2025-01-13 20:11:57 UTC -- Dumping artifacts ... done\n    2025-01-13 20:11:57 UTC -- Dumping pages ...\n    2025-01-13 20:11:57 UTC -- Dumping pages ... done\n    2025-01-13 20:11:57 UTC -- Dumping lfs objects ...\n    2025-01-13 20:11:57 UTC -- Dumping lfs objects ... done\n    2025-01-13 20:11:57 UTC -- Dumping terraform states ...\n    2025-01-13 20:11:57 UTC -- Dumping terraform states ... done\n    2025-01-13 20:11:57 UTC -- Dumping container registry images ... [DISABLED]\n    2025-01-13 20:11:57 UTC -- Dumping packages ...\n    2025-01-13 20:11:57 UTC -- Dumping packages ... done\n    2025-01-13 20:11:57 UTC -- Dumping ci secure files ...\n    2025-01-13 20:11:57 UTC -- Dumping ci secure files ... done\n    2025-01-13 20:11:57 UTC -- Dumping external diffs ...\n    2025-01-13 20:11:57 UTC -- Dumping external diffs ... done\n    2025-01-13 20:11:57 UTC -- Creating backup archive: 1736799112_2025_01_13_17.7.1_gitlab_backup.tar ...\n    2025-01-13 20:11:57 UTC -- Creating backup archive: 1736799112_2025_01_13_17.7.1_gitlab_backup.tar ... done\n    2025-01-13 20:11:57 UTC -- Uploading backup archive to remote storage  ... [SKIPPED]\n    2025-01-13 20:11:57 UTC -- Deleting old backups ... [SKIPPED]\n    2025-01-13 20:11:57 UTC -- Deleting tar staging files ...\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/backup_information.yml\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/db\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/repositories\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/uploads.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/builds.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/artifacts.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/pages.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/lfs.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/terraform_state.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/packages.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/ci_secure_files.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/external_diffs.tar.gz\n    2025-01-13 20:11:57 UTC -- Deleting tar staging files ... done\n    2025-01-13 20:11:57 UTC -- Deleting backups/tmp ...\n    2025-01-13 20:11:57 UTC -- Deleting backups/tmp ... done\n    2025-01-13 20:11:57 UTC -- Warning: Your gitlab.rb and gitlab-secrets.json files contain sensitive data\n    and are not included in this backup. You will need these files to restore a backup.\n    Please back them up manually.\n    2025-01-13 20:11:57 UTC -- Backup 1736799112_2025_01_13_17.7.1 is done.\n    2025-01-13 20:11:57 UTC -- Deleting backup and restore PID file at [/opt/gitlab/embedded/service/gitlab-rails/tmp/backup_restore.pid] ... done"", 'New year, new activities! Hello, GitLab Community! üëã\n\nCheck out the list of trends (and threats) in data protection! Enter 2025 safely!\n\n# üìö News & Resources\n\n**Blog Post üìù| GitLab 17.7 Release** GitLab 17.7 release introduces over 230 improvements. These include: a new Planner user role, auto-resolution policy for vulnerabilities, admin-controlled instance integration allowlists, access token rotation in the UI, and much more! GitLab expressed their gratitude towards the community for 138 contributions. üëâ [Find out more](https://about.gitlab.com/releases/2024/12/19/gitlab-17-7-released/)\xa0\n\n**Blog Post üìù| Data Protection And Backup Predictions For 2025 and beyond** Gartner predicts that by 2028, roughly 75% of organizations will be relying on SaaS applications for backup. Not a surprising statistic when we consider the rising cyber threats and more rigorous regulations. This article provides an overview of data protection trends predicted for 2025 and beyond! üëâ [Full article](https://gitprotect.io/blog/data-protection-and-backup-predictions-for-2025-and-beyond/)\n\n\xa0**Blog Post üìù| Automating with GitLab Duo, Part 3: Validating testing** This article outlines the tests that the author ran while trying to validate the impact of GitLab Duo on their team‚Äôs automated testing. The results gathered from this are discussed and show what has been achieved so far. üëâ [More details](https://about.gitlab.com/blog/2024/12/17/automating-with-gitlab-duo-part-3-validating-testing/)\xa0\n\n\xa0**Blog Post üìù| Transform code quality and compliance with automated processes** As you may know, manual code review may not be enough for DevSecOps-focused teams. GitLab outlines its premium features that address the technical debt and security vulnerability challenges of some of the traditional approaches. Learn more about compliance controls, review systems, and software security. üëâ [Read more](https://about.gitlab.com/blog/2024/12/13/transform-code-quality-and-compliance-with-automated-processes/)\xa0\xa0\n\n**Blog Post üìù| Best Practices for Securing Git LFS on GitHub, GitLab, Bitbucket, and Azure DevOps** As you may know, Git Large File Storage (LFS) is an open-source extension for Git, which can be used to handle versioning of larger files. It makes it easier for a developer to manage data since repositories are optimized - data is stored separately from the repo‚Äôs structure. It is also better to know how to protect this data well. üëâ [More details](https://gitprotect.io/blog/best-practices-for-securing-git-lfs-on-github-gitlab-bitbucket-and-azure-devops/)\n\n# üìÖ Upcoming Events\xa0\n\n**Webcast ü™ê | Transitioning from AWS CodeCommit to GitLab | Jan 23, 2025 | 3 pm GMT | Virtual** This webcast will cover topics from why organizations transition to GitLab, what benefits GitLab brings for DevSecOps and how to ensure a smooth transition. Since AWS CodeCommit has been deprecated, it‚Äôs good to guarantee a smooth migration, while keeping your development work, integrations, and processes secure. Check out expert opinions and best practices for a seamless migration! üëâ [Save your spot](https://page.gitlab.com/webcasts-january23-transitioning-aws-codecommit-to-gitlab.html)\n\n\xa0**Online Event ü™ê| GitLab Hackathon | Jan 23 - Jan 30, 2025 | Virtual** GitLab‚Äôs Hackathon is a great opportunity for devs interested in contributing code, translations, UX designs, and more to GitLab. Not only do you get to participate in things of interest to you, but you can actually improve your skills and knowledge over the 7-day hackathon! There will be prizes for participants and their merge requests. üëâ [Take part](https://about.gitlab.com/community/hackathon/)\n\n\xa0**Online Workshop ü™ê| AI in DevSecOps: Hands-on Workshop | Jan 30, 2025 | 2 pm - 5pm CET** This workshop will revolve around AI use in DevSecOps. Check out how a DevSecOps platform with AI can benefit you. It can improve your workflows, beyond code creation - actually streamline the entire software development lifecycle! üëâ [Secure your spot](https://page.gitlab.com/workshop_January30_DuoAIWorkshop_EMEA.html)\xa0\n\n‚úçÔ∏è *Subscribe to* [*GitProtect DevSecOps X-Ray Newsletter*](https://gitprotect.io/gitprotect-newsletter.html?utm_source=sm&utm_medium=ac) *and always stay tuned for more news!*', 'üéâThe **GitLab Hackathon** is now open!üöÄ  \nWe\'re excited to kick off another week of collaboration and innovation! Checkout our kickoff video [here](https://youtube.com/shorts/pAnjq3Xt7J4) and make sure to follow your progress on the hackathon [leaderboard](https://gitlab-community.gitlab.io/community-projects/merge-request-leaderboard/?&createdAfter=2025-01-23&createdBefore=2025-01-30&mergedBefore=2025-03-02&label=Hackathon).\n\n**Ready to contribute?**  \nContributions to all projects under the [gitlab-org](https://gitlab.com/gitlab-org) and [gitlab-com](https://gitlab.com/gitlab-com) groups qualify for the Hackathon. Additionally, contributions to [GitLab Components](https://gitlab.com/components) qualify.\n\n**Not sure what to work on**?\n\n* Checkout the #contribution\\_opportunities channel on Discord\n* Our teams have curated lists of issues ready for you to tackle:\n   * [gitlab-org/gitlab#510132](https://gitlab.com/gitlab-org/gitlab/-/issues/510132)(some issues qualify for bonuses points!)\n   * [gitlab-org/gitlab#513333](https://gitlab.com/gitlab-org/gitlab/-/issues/513333)\n\n**Need help**?  \nReach out to #contribute or ask for help from our [merge request coaches](https://docs.gitlab.com/ee/development/contributing/merge_request_coaches.html) using ""@gitlab-bot help"" in an issue or MR.\n\n**Want to know more?**  \nVisit the hackathon [page](https://about.gitlab.com/community/hackathon/).\n\n**Remember: MRs must be merged within 30 days to qualify.**']"
2,1,31,1_kubernetes_cluster_app_https,"['kubernetes', 'cluster', 'app', 'https', 'github', 'com', 'service', 'github com', 'https github', 'yaml']","[""**k8sgpt (sandbox)**\n\n[https://github.com/k8sgpt-ai/k8sgpt](https://github.com/k8sgpt-ai/k8sgpt) is a well-known one.\n\n**karpor (kusionstack subproject)**\n\n[https://github.com/KusionStack/karpor](https://github.com/KusionStack/karpor)\n\nIntelligence for Kubernetes. World's most promising Kubernetes Visualization Tool for Developer and Platform Engineering teams\n\n**kube-copilot (personal project from Azure)**\n\n[https://github.com/feiskyer/kube-copilot](https://github.com/feiskyer/kube-copilot) \n\n* Automate Kubernetes cluster operations using ChatGPT (GPT-4 or GPT-3.5).\n* Diagnose and analyze potential issues for Kubernetes workloads.\n* Generate Kubernetes manifests based on provided prompt instructions.\n* Utilize native\xa0`kubectl`\xa0and\xa0`trivy`\xa0commands for Kubernetes cluster access and security vulnerability scanning.\n* Access the web and perform Google searches without leaving the terminal.\n\n**some cost related \\`observibility and analysis\\`**\n\nI did not check if all below projects focus on k8s.\n\n\\- opencost\n\n\\- kubecost\n\n\\- karpenter\n\n\\- crane\n\n\\- infracost\n\nAre there any ai-for-k8s projects that I miss?"", 'If anyone has just started playing with Kubernetes, below project would help them to understand many key concepts around Kubernetes. I just deployed it yesterday and open for feedback on this.\n\nIn this Project , you are required to build a containerized application that consists of a Flask web application and a MySQL database. The two components will be deployed on a public cloud Kubernetes cluster in separate namespaces with proper configuration management using ConfigMaps and Secrets.\n\n# Prerequisite\n\n* Kubernetes Cluster (can be a local cluster like Minikube or a cloud-based one).\n* kubectl installed and configured to interact with your Kubernetes cluster.\n* Docker installed on your machine to build and push the Docker image of the Flask app.\n* Docker Hub account to push the Docker image.\n\n# Setup Architecture\n\nhttps://preview.redd.it/t1xwdp14xcke1.png?width=2288&format=png&auto=webp&s=6f746aceb41b49166efba56b76dcd3ac1bbf9bd5\n\nYou will practically use the following key Kubernetes objects. It will help you understand how these objects can be used in real-world project implementations:\n\n* Deployment\n* HPA\n* ConfigMap\n* Secrets\n* StatefulSet\n* Service\n* Namespace\n\n# Build the Python Flask Application\n\nCreate a [app.py](http://app.py) file with following content\n\n    from\xa0flask\xa0import\xa0Flask, jsonify\n    import\xa0os\n    import\xa0mysql.connector\n    from\xa0mysql.connector\xa0import\xa0Error\n    \n    app\xa0=\xa0Flask(__name__)\n    \n    def\xa0get_db_connection():\n        """"""\n    \xa0\xa0\xa0\xa0Establishes a connection to the MySQL database using environment variables.\n    \xa0\xa0\xa0\xa0Expected environment variables:\n    \xa0\xa0\xa0\xa0\xa0\xa0- MYSQL_HOST\n    \xa0\xa0\xa0\xa0\xa0\xa0- MYSQL_DB\n    \xa0\xa0\xa0\xa0\xa0\xa0- MYSQL_USER\n    \xa0\xa0\xa0\xa0\xa0\xa0- MYSQL_PASSWORD\n    \xa0\xa0\xa0\xa0""""""\n        host\xa0=\xa0os.environ.get(""MYSQL_HOST"", ""localhost"")\n        database\xa0=\xa0os.environ.get(""MYSQL_DB"", ""flaskdb"")\n        user\xa0=\xa0os.environ.get(""MYSQL_USER"", ""flaskuser"")\n        password\xa0=\xa0os.environ.get(""MYSQL_PASSWORD"", ""flaskpass"")\n        \n        try:\n            connection\xa0=\xa0mysql.connector.connect(\n                host=host,\n                database=database,\n                user=user,\n                password=password\n            )\n            if\xa0connection.is_connected():\n                return\xa0connection\n        except\xa0Error\xa0as\xa0e:\n            app.logger.error(f""Error connecting to MySQL: {e}"")\n        return\xa0None\n    \n    u/app.route(""/"")\n    def\xa0index():\n        return\xa0f""Welcome to the Flask App running in {os.environ.get(\'APP_ENV\', \'development\')}\xa0mode!""\n    \n    u/app.route(""/dbtest"")\n    def\xa0db_test():\n        """"""\n    \xa0\xa0\xa0\xa0A simple endpoint to test the MySQL connection.\n    \xa0\xa0\xa0\xa0Executes a query to get the current time from the database.\n    \xa0\xa0\xa0\xa0""""""\n        connection\xa0=\xa0get_db_connection()\n        if\xa0connection\xa0is\xa0None:\n            return\xa0jsonify({""error"": ""Failed to connect to MySQL database""}), 500\n        try:\n            cursor\xa0=\xa0connection.cursor()\n            cursor.execute(""SELECT NOW();"")\n            current_time\xa0=\xa0cursor.fetchone()\n            return\xa0jsonify({\n                ""message"": ""Successfully connected to MySQL!"",\n                ""current_time"": current_time[0]\n            })\n        except\xa0Error\xa0as\xa0e:\n            return\xa0jsonify({""error"": str(e)}), 500\n        finally:\n            if\xa0connection\xa0and\xa0connection.is_connected():\n                cursor.close()\n                connection.close()\n    \n    if\xa0__name__\xa0==\xa0""__main__"":\n        debug_mode\xa0=\xa0os.environ.get(""DEBUG"", ""false"").lower() ==\xa0""true""\n        app.run(host=""0.0.0.0"", port=5000, debug=debug_mode)\n\n# Create a Dockerfile for the app\n\n    FROM\xa0python:3.9-slim\n    \n    #\xa0Install ping (iputils-ping) for troubleshooting\n    RUN\xa0apt-get update && apt-get install -y iputils-ping && rm -rf /var/lib/apt/lists/*\n    \n    WORKDIR\xa0/app\n    COPY\xa0requirements.txt .\n    RUN\xa0pip install --upgrade pip && pip install --no-cache-dir -r requirements.txt\n    COPY\xa0app.py .\n    \n    EXPOSE\xa05000\n    ENV\xa0FLASK_APP=app.py\n    \n    CMD\xa0[""python"", ""app.py""]\n\n# Build and Push the docker Image\n\n    docker build -t becloudready/my-flask-app\n\n# Login to DockerHub\n\n    docker login\n\nIt will show a 6 digit Code, which you need to enter to following URL\n\n[https://login.docker.com/activate](https://login.docker.com/activate)\n\nPush the Image to DockerHub\n\n    docker push becloudready/my-flask-app\n\nYou should be able to see the Pushed Image\n\n# Flask Deployment (flask-deployment.yaml)\n\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: flask-deployment\n      namespace: flask-app\n      labels:\n        app: flask\n    spec:\n      replicas: 2\n      selector:\n        matchLabels:\n          app: flask\n      template:\n        metadata:\n          labels:\n            app: flask\n        spec:\n          containers:\n          - name: flask\n            image: becloudready/my-flask-app:latest  #\xa0Replace with your Docker Hub image name.\n            ports:\n            - containerPort: 5000\n            env:\n            - name: APP_ENV\n              valueFrom:\n                configMapKeyRef:\n                  name: flask-config\n                  key: APP_ENV\n            - name: DEBUG\n              valueFrom:\n                configMapKeyRef:\n                  name: flask-config\n                  key: DEBUG\n            - name: MYSQL_DB\n              valueFrom:\n                configMapKeyRef:\n                  name: flask-config\n                  key: MYSQL_DB\n            - name: MYSQL_HOST\n              valueFrom:\n                configMapKeyRef:\n                  name: flask-config\n                  key: MYSQL_HOST\n            - name: MYSQL_USER\n              valueFrom:\n                secretKeyRef:\n                  name: db-credentials\n                  key: username\n            - name: MYSQL_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: db-credentials\n                  key: password\n\n# Flask Service (flask-svc.yaml)\n\n    apiVersion: v1\n    kind: Service\n    metadata:\n      name: flask-svc\n      namespace: flask-app\n    spec:\n      selector:\n        app: flask\n      type: LoadBalancer\n      ports:\n      - port: 80\n        targetPort: 5000\n\n# ConfigMap for Flask App (flask-config.yaml)\n\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: flask-config\n      namespace: flask-app\n    data:\n      APP_ENV: production\n      DEBUG: ""false""\n      MYSQL_DB: flaskdb\n      MYSQL_HOST: mysql-svc.mysql.svc.cluster.local\n\n# Namespaces (namespaces.yaml)\n\n    apiVersion: v1\n    kind: Namespace\n    metadata:\n      name: flask-app\n    ---\n    apiVersion: v1\n    kind: Namespace\n    metadata:\n      name: mysql\n\n# \n\n# Secret for DB Credentials (db-credentials.yaml)\n\n    kubectl create secret generic db-credentials \\\n      --namespace=flask-app \\\n      --from-literal=username=flaskuser \\\n      --from-literal=password=flaskpass \\\n      --from-literal=database=flaskdb\n\n# Setup and Configure MySQL Pods\n\n# ConfigMap for MySQL Init Script (mysql-initdb.yaml)\n\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: mysql-initdb\n      namespace: mysql\n    data:\n      initdb.sql: |\n    \xa0\xa0\xa0\xa0CREATE DATABASE IF NOT EXISTS flaskdb;\n    \xa0\xa0\xa0\xa0CREATE USER \'flaskuser\'@\'%\' IDENTIFIED BY \'flaskpass\';\n    \xa0\xa0\xa0\xa0GRANT ALL PRIVILEGES ON flaskdb.* TO \'flaskuser\'@\'%\';\n    \xa0\xa0\xa0\xa0FLUSH PRIVILEGES;\n\n# MySQL Service (mysql-svc.yaml)\n\n    apiVersion: v1\n    kind: Service\n    metadata:\n      name: mysql-svc\n      namespace: mysql\n    spec:\n      selector:\n        app: mysql\n      ports:\n      - port: 3306\n        targetPort: 3306\n\n# MySQL StatefulSet (mysql-statefulset.yaml)\n\n    apiVersion: apps/v1\n    kind: StatefulSet\n    metadata:\n      name: mysql-statefulset\n      namespace: mysql\n      labels:\n        app: mysql\n    spec:\n      serviceName: ""mysql-svc""\n      replicas: 1\n      selector:\n        matchLabels:\n          app: mysql\n      template:\n        metadata:\n          labels:\n            app: mysql\n        spec:\n          initContainers:\n          - name: init-clear-mysql-data\n            image: busybox\n            command: [""sh"", ""-c"", ""rm -rf /var/lib/mysql/*""]\n            volumeMounts:\n            - name: mysql-persistent-storage\n              mountPath: /var/lib/mysql\n          containers:\n          - name: mysql\n            image: mysql:5.7\n            ports:\n            - containerPort: 3306\n              name: mysql\n            env:\n            - name: MYSQL_ROOT_PASSWORD\n              value: rootpassword   #\xa0For production, use a Secret instead.\n            - name: MYSQL_DATABASE\n              value: flaskdb\n            - name: MYSQL_USER\n              value: flaskuser\n            - name: MYSQL_PASSWORD\n              value: flaskpass\n            volumeMounts:\n            - name: mysql-persistent-storage\n              mountPath: /var/lib/mysql\n            - name: initdb\n              mountPath: /docker-entrypoint-initdb.d\n          volumes:\n          - name: initdb\n            configMap:\n              name: mysql-initdb\n      volumeClaimTemplates:\n      - metadata:\n          name: mysql-persistent-storage\n        spec:\n          accessModes: [ ""ReadWriteOnce"" ]\n          resources:\n            requests:\n              storage: 1Gi\n          storageClassName: do-block-storage\n\n# Deploy to Kubernetes\n\n* Create Namespaces:\n\n\n\n    kubectl apply -f namespaces.yaml\n\n* Deploy ConfigMaps and Secrets:\n\n\n\n    kubectl apply -f flask-config.yaml \n    kubectl apply -f mysql-initdb.yaml \n    kubectl apply -f db-credentials.yaml\n\n* Deploy MySQL:\n\n\n\n    kubectl apply -f mysql-svc.yaml \n    kubectl apply -f mysql-statefulset.yaml\n\n* Deploy Flask App:\n\n\n\n    kubectl apply -f flask-deployment.yaml \n    kubectl apply -f flask-svc.yaml\n\nTest the Application\n\n\n\n`kubectl get svc -n flask-app`   \n`NAME        TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)        AGE`   \n`flask-svc   LoadBalancer   10.109.112.171   146.190.190.51   80:32618/TCP   2m53s`  \n  \n  \n`curl` [`http://146.190.190.51/dbtest`](http://146.190.190.51/dbtest) `{""current_time"":""Wed, 19 Feb 2025 21:37:57 GMT"",""message"":""Successfully connected to MySQL!""}`\n\n# Troubleshooting\n\nUnable to connect to MySQL from Flask App\n\nLogin to the Flask app pod to ensure all values are loaded properly\n\n    kubectl exec -it flask-deployment-64c8955d64-hwz7m -n flask-app -- bash\n    \n    root@flask-deployment-64c8955d64-hwz7m:/app# env | grep -i mysql\n    MYSQL_DB=flaskdb\n    MYSQL_PASSWORD=flaskpass\n    MYSQL_USER=flaskuser\n    MYSQL_HOST=mysql-svc.mysql.svc.cluster.local\n\n# Testing\n\n* Flask App:Access the external IP provided by the LoadBalancer service to verify the app is running.\n* Database Connection:Use the /dbtest endpoint of the Flask app to confirm it connects to MySQL.\n* Troubleshooting:Use kubectl logs and kubectl exec to inspect pod logs and verify environment variables.', '## Why KubeVPN?\n\nIn the Kubernetes era, developers face a critical conflict between **cloud-native complexity** and **local development\nagility**. Traditional workflows force developers to:\n\n1. Suffer frequent `kubectl port-forward`/`exec` operations\n2. Set up mini Kubernetes clusters locally (e.g., minikube)\n3. Risk disrupting shared dev environments\n\nKubeVPN solves this through **cloud-native network tunneling**, seamlessly extending Kubernetes cluster networks to\nlocal machines with three breakthroughs:\n\n- üöÄ **Zero-Code Integration**: Access cluster services without code changes\n- üíª **Real-Environment Debugging**: Debug cloud services in local IDEs\n- üîÑ **Bidirectional Traffic Control**: Route specific traffic to local or cloud\n\n![KubeVPN Architecture](https://raw.githubusercontent.com/kubenetworks/kubevpn/master/samples/flat_log.png)\n\n## Core Capabilities\n\n### 1. Direct Cluster Networking\n\n```bash\nkubevpn connect\n```\n\nInstantly gain:\n\n- ‚úÖ Service name access (e.g., `productpage.default.svc`)\n- ‚úÖ Pod IP connectivity\n- ‚úÖ Native Kubernetes DNS resolution\n\n```shell\n‚ûú curl productpage:9080 # Direct cluster access\n<!DOCTYPE html>\n<html>...</html>\n```\n\n### 2. Smart Traffic Interception\n\nPrecision routing via header conditions:\n\n```bash\nkubevpn proxy deployment/productpage --headers user=dev-team\n```\n\n- Requests with `user=dev-team` ‚Üí Local service\n- Others ‚Üí Original cluster handling\n\n### 3. Multi-Cluster Mastery\n\nConnect two clusters simultaneously:\n\n```bash\nkubevpn connect -n dev --kubeconfig ~/.kube/cluster1  # Primary\nkubevpn connect -n prod --kubeconfig ~/.kube/cluster2 --lite # Secondary\n```\n\n### 4. Local Containerized Dev\n\nClone cloud pods to local Docker:\n\n```bash\nkubevpn dev deployment/authors --entrypoint sh\n```\n\nLaunched containers feature:\n\n- üåê Identical network namespace\n- üìÅ Exact volume mounts\n- ‚öôÔ∏è Matching environment variables\n\n## Technical Deep Dive\n\nKubeVPN\'s three-layer architecture:\n\n| Component           | Function                     | Core Tech                  |\n|---------------------|------------------------------|----------------------------|\n| **Traffic Manager** | Cluster-side interception    | MutatingWebhook + iptables |\n| **VPN Tunnel**      | Secure local-cluster channel | tun device + WireGuard     |\n| **Control Plane**   | Config/state sync            | gRPC streaming + CRDs      |\n\n```mermaid\ngraph TD\n    Local[Local Machine] -->|Encrypted Tunnel| Tunnel[VPN Gateway]\n    Tunnel -->|Service Discovery| K8sAPI[Kubernetes API]\n    Tunnel -->|Traffic Proxy| Pod[Workload Pods]\n    subgraph K8s Cluster\n        K8sAPI --> TrafficManager[Traffic Manager]\n        TrafficManager --> Pod\n    end\n```\n\n## Performance Benchmark\n\n100QPS load test results:\n\n| Scenario      | Latency | CPU Usage | Memory |\n|---------------|---------|-----------|--------|\n| Direct Access | 28ms    | 12%       | 256MB  |\n| KubeVPN Proxy | 33ms    | 15%       | 300MB  |\n| Telepresence  | 41ms    | 22%       | 420MB  |\n\nKubeVPN outperforms alternatives in overhead control.\n\n## Getting Started\n\n### Installation\n\n```bash\n# macOS/Linux\nbrew install kubevpn\n\n# Windows\nscoop install kubevpn\n\n# Via Krew\nkubectl krew install kubevpn/kubevpn\n```\n\n### Sample Workflow\n\n1. **Connect Cluster**\n\n```bash\nkubevpn connect --namespace dev\n```\n\n2. **Develop & Debug**\n\n```bash\n# Start local service\n./my-service &\n\n# Intercept debug traffic\nkubevpn proxy deployment/frontend --headers x-debug=true\n```\n\n3. **Validate**\n\n```bash\ncurl -H ""x-debug: true"" frontend.dev.svc/cluster-api\n```\n\n## Ecosystem\n\nKubeVPN\'s growing toolkit:\n\n- üîå **VS Code Extension**: Visual traffic management\n- üß© **CI/CD Pipelines**: Automated testing/deployment\n- üìä **Monitoring Dashboard**: Real-time network metrics\n\nJoin developer community:\n\n```bash\n# Contribute your first PR\ngit clone https://github.com/kubenetworks/kubevpn.git\nmake kubevpn\n```\n\n---\n\n> Project URL: [https://github.com/kubenetworks/kubevpn](https://github.com/kubenetworks/kubevpn)  \n> Documentation: [Complete Guide](https://github.com/kubenetworks/kubevpn/wiki)  \n> Support: Slack #kubevpn\n\nWith KubeVPN, developers finally enjoy cloud-native debugging while sipping coffee ‚òïÔ∏èüöÄ']"
3,2,27,2_cd_ci cd_ci_use,"['cd', 'ci cd', 'ci', 'use', 'terraform', 'github', 'docker', 'https', 'services', 'just']","[""Hi, I wonder how ci/cd should work and what instruments i have to use. \n\nI'm making my backend part of pet project and use docker. So i want to setup ci/cd fot my project and automatically integrate my new code to docker container. I'm confusing with a ci/cd pipeline. What tools i should use and how my new code will delivery to existing one. \n\nCan someone explain me that or maybe send any kind of guids. (videos, text tutorials, etc) Thanks in advance for any help."", 'Hello r/cicd,\n\nI work for a section of a university, that helps researchers, well... research i guess.  \nWe store data, grant access, manage the infrastrukture and assist in the researchers projects.\n\nAs you can imagine, these task lead to projects we do ourself. One of these projects was now handed down to me and 3 others, trying to answere the question, ""How can Ci/Cd help us?"" We are about 120 people, working on seperate tasks as described above. While we are that many people. usually only 2 to 4 people are assigned to projects, be that with other researchers or internal tasks such us maintaining the out-of-office tool (2 people) or the infrastukture that hosts the data and grants access to researcher groups (4 people)\n\nSo you see, every project that would benefit from Ci/Cd is itself smaller and most of the time does not lead to grander picture but is completed and archived after the project ends. Usually the documentation is then put onto an internal wiki for later re-use.\n\nBack to the question, ""How can Ci/Cd help us?"":  \nTeam Ci/Cd has met 3 times now, trying to understand where people in our organisation are using Ci/Cd. We found some attempts and some half automated pipelines, but not the complete picture. We started to ask if the complete automated pipeline is even something we would want.  \nThis is not my question to you.\n\nWhen should you use Ci/Cd?  \nIs there a minimum size of project members that should be reached to use the complete Ci/Cd pipeline?  \nIs it not related to project members but project complexity?  \nDo you always try to automate everything or have had a project yourself where you started with Ci/Cd but at the end decided to leave in a manual check?\n\nPS: Now that i have written all of this, I also wonder, is it worth to change the way an organisation works to make Ci/Cd lucrative or is better to ""not change a running system""?\n\nKind regards', 'TLDR: What\'s the best practice for managing infra with custom Docker based images using Terraform?\n\nWe primarily use GCP and for a lot of simple services we use Cloud Run with GAR (Google Artifact Registry) to store the Docker images.\n\nTo manage the infra, we generally use Terraform and we use GitHub Actions to do CI & CD.\n\n\n\nDeployments to new environments comprise of the following steps:\n\n1) \\[Terraform\\] Create a new GAR repository that Docker can push to\n\n2) \\[Docker\\] Build and push the Docker Image on the newly created GAR and then\n\n3) \\[Terraform\\] Deploy the Cloud Run service which uses the GAR, along side any other infrastructure we might need.\n\nThis 3 step process is usually how our CD (GitHub Actions) is structured and how our ""local"" dev (i.e. personal dev projects) works, both usually running with [just](https://github.com/casey/just) as the command runner. \n\nTerraform needs to have a ""bootstrap"" environment which gets deployed in the first step, separate from the ""main"" one used in the third. Although, instead of using a separate bootstrap environment, you can also use -target to apply just the GAR but that has its own downsides imo (not a fan of partial apply, especially if bootstrap involves additional steps such as service account creation and IAM role assignment).\n\n  \nIt\'s possible to avoid having two Terraform apply steps by doing one of the following:\n\n\\- Deploy the Cloud Run services manually using the gcloud CLI - but then you cannot manage it well via Terraform which can be problematic for certain situations.\n\n\\- Perform the bootstrap separately (perhaps manual operations?) so normal work doesn\'t require it - but this sounds like a recipe for non reproducible infra - might make disaster recovery painful\n\n\\- Run the docker commands as part of some terraform operator (using either a null resource with local exec or perhaps an existing provider such as [kreuzwerker/terraform-provider-docker](https://github.com/kreuzwerker/terraform-provider-docker)), but this might be slow for repetitive work and might just not integrate that well with Terraform\n\n\n\nAny suggestions how we can do this better? For trivial services it\'s a lot of boilerplate stuff that needs to be written, and it just drains the fun out of it tbh. With some work I suppose it\'s possible to reuse some of the code, but we might put some unnecessary constrains and abstracting it right might take some work.\n\nIn a totally different world from my day job, my hobby NextJS apps are trivial to develop and a lot more fun. I can focus on the app code instead of all this samey stuff which adds 0 business value.']"
4,3,17,3_devops_experience_learning_learn,"['devops', 'experience', 'learning', 'learn', 'tools', 'ai', 'job', 'python', 'devops tools', 'like']","[""Hi fellas,\n\nI recently posted about the AI threat in DevOps. I understood from the responses that AI will definitely be a threat if I don't upskill myself with all the tools and their AI integration.\n\nTo work on this, I want to know some resources/projects that can be used hands-on to learn and understand more about DevOps tools with minimal cost.\n\nI currently have a KodeKloud Pro subscription, so the learning part is covered (although I'm not relying on it completely).\n\nWhat I want from all of you is help getting started with the learning. Please share low-cost resources or projects with a good learning curve about DevOps tools.  If you can provide YouTube videos that create complete projects from scratch, that would be really helpful."", 'I started off learning about DevOps soon after I got into self hosting and running my own homelab, fast forward a few years this has become my addiction. I work with VoIP currently and play around with Linux a bit for work but nothing with containers or DevOps tools, so i have just been learning with my homelab.\n\nAnyways, Im sick of VoIP and my current role, and would like to start applying for some Jr DevOps roles but am curious from the people who actually do this as a job if you would think I am prepared enough just based on my homelab.\n\nPersonally I think i need to get better with Ansible, Kubernetes, adding more things to Terraform/OpenTofu, and learning coding languages, this is what I am working on currently.\n\nAll of the config can be located here [https://git.mafyuh.dev/mafyuh/iac](https://git.mafyuh.dev/mafyuh/iac) or on Github here [https://github.com/Mafyuh/iac](https://github.com/Mafyuh/iac)\n\nPlease critique and let me know what you think, this is my first time ever posting in DevOps so dont really know what to expect but id love to hear it all, good or bad. Thank you', ""Hello everyone.\nI'm currently tagged as a DevOps Engineer having following experience: \nAzure Webapp and VMs, Azure DevOps.\nI'm having 4.2 YOE since I started my career in IT industry. \nI don't have any kind of experience in K8s or docker or monitoring or jenkins or any other tools.\n\nI want to know how much should I be afraid of this AI impact?\nShould I change my domain from devops to data engineer or anything else?\nWhich DevOps Zone is AI impact proof(so that our job won't affeft much)\n\nI'm really afraid and in panic mode right now as people are getting laid off and these CEOs and big companies are coming up new thing every week that AI will impact our job.\nPlease guys HELP ME!!""]"
5,4,12,4_shared_gitlab_plan_bash,"['shared', 'gitlab', 'plan', 'bash', 'ci', 'pipeline', 'gitlab ci', 'job', 'file', 'repository']","['We are trying to disable that Auto DevOps feature on some of our projects and it doesn\'t seem to take effect.  We followed the instructions in [https://docs.gitlab.com/ee/topics/autodevops/](https://docs.gitlab.com/ee/topics/autodevops/) by unchecking the Default to AutoDev Ops pipeline box found in the projects Settings>CI/CD>Auto DevOps section.  However the pipeline is still starting automatically on every commit.  Does the fact that a .gitlab-ci.yml file exists at the root of the repository override the setting?\n\nEDIT: Here is a summary of what we are tying to do\n\n* Use Gitlab\'s CI/CD pipeline for only manual starts with the Run pipeline button\n* Use pre-filled variables that we want displayed in the run pipeline form with scoped options.  We got this working.\n* We do not want the pipeline to auto start on commits.\n\nHere is what we tried so far\n\n* Unchecked the project CI/CD Auto DevOps setting\n   * Still builds on commit\n* Used a different template file name at the root\n   * We were prompted to set up a pipeline with the default .gitlab-ci.yml file\n   * We could not run any pipelines\n* Used a different template file and set it in the project CI/CD general pipeline settings\n   * It started auto building on commit again\n* Added a workflow if rule where the CI\\_PIPELINE\\_SOURCE is ""web"" then run\n   * This seems to work however if someone misses this item in the template then it will auto build again.\n\nIs there a way in GitLab CI/CD to use the pipeline but have Auto DevOps disabled by default?  If so at what level can it be done at (System, Group, Project, etc)?', 'hey folks\n\nI started to try to create dynamic pipelines with Gitlab using `parallel:matrix`, but I am struggling to make it dynamic.\n\nMy current job look like this:\n\n    #.gitlab-ci.yml\n    include:\n    \xa0 - local: "".gitlab/terraform.gitlab-ci.yml""\n    \n    variables:\n    \xa0 STORAGE_ACCOUNT: ${TF_STORAGE_ACCOUNT}\n    \xa0 CONTAINER_NAME: ${TF_CONTAINER_NAME}\n    \xa0 RESOURCE_GROUP: ${TF_RESOURCE_GROUP}\n    \n    workflow:\n    \xa0 rules:\n    \xa0 \xa0 - if: $CI_COMMIT_BRANCH == ""main""\n    \xa0 \xa0 - if: $CI_PIPELINE_SOURCE == ""merge_request_event""\n    \xa0 \xa0 - if: $CI_PIPELINE_SOURCE == ""web""\n    \n    prepare:\n    \xa0 image: jiapantw/jq-alpine\n    \xa0 stage: .pre\n    \xa0 script: |\n    \xa0 \xa0 # Create JSON array of directories\n    \xa0 \xa0 DIRS=$(find . -name ""*.tf"" -type f -print0 | xargs -0 -n1 dirname | sort -u | sed \'s|^./||\' | jq -R -s -c \'split(""\\n"")[:-1] | map(.)\')\n    \xa0 \xa0 echo ""TF_DIRS=$DIRS"" >> terraform_dirs.env\n    \xa0 artifacts:\n    \xa0 \xa0 reports:\n    \xa0 \xa0 \xa0 dotenv: terraform_dirs.env\n    \n    .dynamic_plan:\n    \xa0 extends: .plan\n    \xa0 stage: plan\n    \xa0 parallel:\n    \xa0 \xa0 matrix:\n    \xa0 \xa0 \xa0 - DIRECTORY: ${TF_DIRS} \xa0# Will be dynamically replaced by GitLab with array values\n    \xa0 rules:\n    \xa0 \xa0 - if: $CI_PIPELINE_SOURCE == ""merge_request_event""\n    \xa0 \xa0 - if: $CI_COMMIT_BRANCH == ""main""\n    \xa0 \xa0 - if: $CI_PIPELINE_SOURCE == ""web""\n    \n    .dynamic_apply:\n    \xa0 extends: .apply\n    \xa0 stage: apply\n    \xa0 parallel:\n    \xa0 \xa0 matrix:\n    \xa0 \xa0 \xa0 - DIRECTORY: ${TF_DIRS} \xa0# Will be dynamically replaced by GitLab with array values\n    \xa0 rules:\n    \xa0 \xa0 - if: $CI_COMMIT_BRANCH == ""main""\n    \xa0 \xa0 - if: $CI_PIPELINE_SOURCE == ""web""\n    \n    stages:\n    \xa0 - .pre\n    \xa0 - plan\n    \xa0 - apply\n    \n    plan:\n    \xa0 extends: .dynamic_plan\n    \xa0 needs:\n    \xa0 \xa0 - prepare\n    \n    apply:\n    \xa0 extends: .dynamic_apply\n    \xa0 needs:\n    \xa0 \xa0 - job: plan\n    \xa0 \xa0 \xa0 artifacts: true\n    \xa0 \xa0 - prepare\n\n\n\nand the local template looks like this:\n\n    # .gitlab/terraform.gitlab-ci.yml\n    .terraform_template: &terraform_template\n      image: hashicorp/terraform:latest\n      variables:\n        TF_STATE_NAME: ${CI_COMMIT_REF_SLUG}\n        TF_VAR_environment: ${CI_ENVIRONMENT_NAME}\n      before_script:\n        - export\n        - cd ""${DIRECTORY}""  # Added quotes to handle directory names with spaces\n        - terraform init \\\n          -backend-config=""storage_account_name=${STORAGE_ACCOUNT}"" \\\n          -backend-config=""container_name=${CONTAINER_NAME}"" \\\n          -backend-config=""resource_group_name=${RESOURCE_GROUP}"" \\\n          -backend-config=""key=${DIRECTORY}.tfstate"" \\\n          -backend-config=""subscription_id=${ARM_SUBSCRIPTION_ID}"" \\\n          -backend-config=""tenant_id=${ARM_TENANT_ID}"" \\\n          -backend-config=""client_id=${ARM_CLIENT_ID}"" \\\n          -backend-config=""client_secret=${ARM_CLIENT_SECRET}""\n    \n    .plan:\n      extends: .terraform_template\n      script:\n        - terraform plan -out=""${DIRECTORY}/plan.tfplan""\n      artifacts:\n        paths:\n          - ""${DIRECTORY}/plan.tfplan""\n        expire_in: 1 day\n    \n    .apply:\n      extends: .terraform_template\n      script:\n        - terraform apply -auto-approve ""${DIRECTORY}/plan.tfplan""\n      dependencies:\n        - plan\n\nNo matter how hard I try to make it work, it only generates a single job with plan, named \\``plan: [${TF_DIRS}]`  and another with apply.\n\nIf I change this line and make it static: `- DIRECTORY: ${TF_DIRS}`, like this: `- DIRECTORY: [""dir1"",""dir2"",""dirN""]`. it does exactly what I want.\n\nThe question is: is `parallel:matrix` ever going to work with a dynamic value or not?   \nThe second question is: should I move to any other approach already?\n\nThx in advance.', '**Hi GitLab Community,**\n\nI\'m looking for advice on how to structure my GitLab CI/CD pipelines when sharing functionality across repositories. Here‚Äôs my use case:\n\n### The Use Case\nI have two repositories:  \n- **repository1**: A project-specific repository. There will be multiple Repositorys like this including functionality from the ""gitlab-shared"" Repository\n- **gitlab-shared**: A repository for shared CI/CD functionality.\n\nIn **Repository 1**, I include shared functionality from the **GitLab Shared Repository** using `include: project` in my `.gitlab-ci.yml`:\n\n```yaml\n# ""repository1"" including the ""gitlab-shared"" repository for shared bash functions\ninclude:\n  # Include the shared library for common CI/CD functions\n  - project: \'mygroup/gitlab-shared\'\n    ref: main\n    file: \n      - \'ci/common.yml\' # Includes shared functionality such as bash exports\n```\n\nThe `common.yml` in the **GitLab Shared Repository** defines a hidden job to set up bash functions:\n\n```yaml\n# Shared functionality inside ""gitlab-shared""\n.setup_utility_functions:\n  script:\n    - |\n      function some_function(){\n        echo ""does some bash stuff that is needed in many repositories""\n      }\n      function some_function2(){\n        echo ""also does some complicated stuff""\n      }\n```\n\nIn **Repository 1**, I make these shared bash functions available like this:\n\n```yaml\n# Using the shared setup function to export bash functions in ""repository1""\ndefault:\n  before_script:\n    - !reference [.setup_utility_functions, script]\n```\n\nThis works fine, but here\'s my **problem**:\n\n---\n\n### The Problem\nAll the bash code for the shared functions is written inline in `common.yml` in the **GitLab Shared Repository**. I‚Äôd much prefer to extract these bash functions into a dedicated bash file for better readability in my IDE.\n\nHowever, because `include: project` only includes `.yml` files, I cannot reference bash files from the shared repository. The hidden job `.setup_utility_functions` in **Repository 1** fails because the bash file is not accessible.\n\n---\n\n### My Question\nIs there a better way to structure this? Ideally, I\'d like to:  \n1. Write the bash functions in a bash file in the **GitLab Shared Repository**.  \n2. Call this bash file from the hidden job `.setup_utility_functions` in **Repository 1**.\n\nRight now, I‚Äôve stuck to simple bash scripts for their readability and simplicity, but the lack of support for including bash files across repositories has become a little ugly.\n\nAny advice or alternative approaches would be greatly appreciated!\n\nThanks in advance! üòä']"
6,5,9,5_commit_repository_git_folder,"['commit', 'repository', 'git', 'folder', 'new', 'want', 'history', 'reset', 'commits', 'change']","[""I created a new folder to get the folder system but now I somehow deleted it trying to commit from VScode, because I had opened the folder and it wasn't commiting to github, so I opened a new one and then deleted the one that wasn't commiting and it deleted everything but the README file when I commited that one.\n\nI also didn't have all the folders on GitHub idk why, so I was also trying to fix that\n\nI had been using the terminal before this. I don't wanna create a new folder and start from scratch, I want to learn how to fix problems like this. I've already googled and they all want me to create a new repo\n\nWhen I use\n\ngit add FOLDERNAME/\n\nit just tells me I have nothing to commit"", ""Hello,  \nI have a repository with lfs enabled that had its history entirely removed. Its now a single commit with a single file. This happened a few month ago, and I couldnt find anybody with a local clone. I'm trying to recover/fix the history.\n\nI have tried `git log --oneline --all --graph --decorate --reflog`\nwhich gives me the latest commit and all the history earlier than the lastest merge request, but there are several commit that should be here. i.e.\n\n> *latest commit  \n> Here commits are missing  \n> * latest merge request\n> *rest of the history\n\n\nI have found on gitlab a commit hash relatively recent (almost the latest before the history deletion) and it still have almost all the history I need. I have tried to reset or checkout this commit without success (with `git reset --soft commit_sha` and `git checkout -b branch commit_sha`)\n\nAny idea how to reset to this commit?"", 'I\'m new to Git--I\'ve used GitHub to share code and fork projects but don\'t know really anything about how Git works ""under the hood""?\n\nSo let\'s say that I want to clone a previous release of a repository, that may be 5+ years old, because after that, the owner of that repository made a change that I don\'t want to have in my version. Just doing this, as I understand, requires forking the repository as a whole and then ""checking out"" a previous commit of the repository. Let\'s say I manage to do this--so now I have a repository that has my own ""snapshot"" of that other repository as it was back then, that I can then modify in a new direction.\n\nNow let\'s say I want to incorporate into my own fork some changes to the original repository that happened more recently, that are in a different part of the code that doesn\'t affect the files modified by the change I\'m avoiding by cloning the earlier version. So I want to do something like merging, but effectively ""write protect"" the part that I want to keep from the earlier version, i.e. reject the commit(s) that introduced the unwanted change, and I would also need to have some sort of warning issued, and an option to reject, if potential commits to OTHER files reference functions, classes, etc. that were introduced by the change I\'m omitting, and that therefore don\'t exist in my branch(\\*).\n\nDoes Git allow this kind of filtering when merging? If not, is there a tool available that\'s made for this use case? And as a more ""philosophical"" question, is the better way to go about this to clone a newer version, then manually ""roll back"" just the change I want to skip by copying the old versions of the relevant files in place of the new ones, and then search *from there* for references to the new versions and modify those to work with the old versions of the rolled-back files?\n\n(\\*)I\'m well aware that this is *necessary*, but not *sufficient*, to detect changes that can\'t be made without breaking the code. Being able to check with certainty that code will work not only would require compiler-level knowledge of the relevant languages, it\'s likely NP complete, and in any case is FAR above what could be expected from something like Git. Even recursively detecting commits with broken references TO broken references would likely be hard--I\'m only aiming for ""one level deep"" filtering based on string searching for direct references to code that was added/modified in the skipped change.']"
