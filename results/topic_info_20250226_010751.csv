,Topic,Count,Name,Representation,Representative_Docs
0,-1,17,-1_gitlab_false_drop_build,"['gitlab', 'false', 'drop', 'build', 'https', 'google', 'ci', 'template', 'png', 'files']","[""Hi everyone,\n\nI've encountered some unexpected behavior in my GitLab instance and wanted to check if anyone has experienced something similar.\n\nLooking at my PostgreSQL logs, I noticed a sequence of DROP TABLE commands targeting various tables (e.g., ci\\_runners\\_e59bb2812d, sprints, ai\\_conversation\\_threads, etc.), followed by an attempt to DROP DATABASE gitlabhq\\_production. Here‚Äôs a snippet from the logs:\n\n2025-02-13 17:27:45.800 UTC \\[60997\\] postgres@gitlabhq\\_production LOG:  statement: DROP TABLE IF EXISTS ci\\_runners\\_e59bb2812d CASCADE;\n\n2025-02-13 17:27:45.953 UTC \\[60997\\] postgres@gitlabhq\\_production LOG:  statement: DROP TABLE IF EXISTS sprints CASCADE;\n\n...\n\n2025-02-13 17:27:47.085 UTC \\[61024\\] postgres@postgres LOG:  statement: DROP DATABASE gitlabhq\\_production;\n\n2025-02-13 17:27:47.345 UTC \\[61024\\] postgres@postgres LOG:  AUDIT: SESSION,1,1,DDL,DROP DATABASE,,,DROP DATABASE gitlabhq\\_production;,<not logged>\n\nIt looks like something attempted to drop the entire GitLab database. Fortunately, the DROP DATABASE postgres; command failed due to it being in use.\n\nMy Questions:\n\nIs this expected behavior? Could this be part of a GitLab cleanup or maintenance process?\n\nHas anyone seen similar logs? Could this be due to an automatic migration, or does it suggest an external issue?\n\nAny recommendations on where to investigate further?\n\nI‚Äôd appreciate any insights or experiences you can share. Thanks in advance!"", ""Hey everyone,\n\nI'm looking for a reliable tool that can detect Personally Identifiable Information (PII)‚Äîsuch as names, phone numbers, bank account details‚Äîand other sensitive data in both code repositories and images within GitLab.\n\nIdeally, the tool should:\n\nIntegrate with GitLab CI/CD for automated scanning\n\nSupport SAST .gitlab-ci.yml, SARIF files, or any other format to view detailed reports\n\nDetect PII and SPI across code, commits, and Git history\n\nI‚Äôm aware of GitLab‚Äôs SAST capabilities, but I haven't seen any options to add custom regex-based rulesets for PII/SPI detection.\n\nI‚Äôve come across TruffleHog and GitLeaks, but I‚Äôd love to hear about any other recommendations, especially tools that generate detailed, viewable reports in GitLab.\n\nHas anyone implemented a similar solution for GitLab reporting in their workflow? Any insights or best practices would be greatly appreciated."", 'On Gitlab, I want it so that my markdown files and other files of different types count as different languages on the summary page of my repo.\n\n[The current language bar looks like this for me](https://preview.redd.it/3wefgcsxonfe1.png?width=349&format=png&auto=webp&s=563e0d644e22efd9fa7056d6ef075ac2b7a0aa08)\n\nBut then I have my gitattributes filled out to recognize these other file types\n\n[.gitattributes](https://preview.redd.it/la8fkt13pnfe1.png?width=1310&format=png&auto=webp&s=95c9b5ea52bd11662f227afcb29ce6c38483cefb)\n\n    # Please show these langauges in stats\n    *.txt linguist-detectable=true linguist-language=Text linguist-documentation=false linguist-generated=false linguist-vendored=false\n    *.cbp linguist-detectable=true linguist-language=XML linguist-documentation=false linguist-generated=false linguist-vendored=false\n    *.md linguist-detectable=true linguist-language=Markdown linguist-documentation=false linguist-generated=false linguist-vendored=false\n    *.yml linguist-detectable=true linguist-language=YAML linguist-documentation=false linguist-generated=false linguist-vendored=false\n\nHere are the files that I have in my project, so I think it should be recognizing my .cbp and my text files and readme\n\n[Files in my project](https://preview.redd.it/6noxu1mdpnfe1.png?width=242&format=png&auto=webp&s=f7e767cef909756f40290aa4ad643ce06938b95f)\n\nAny help would be appreciated']"
1,0,41,0_gitlab_2025_utc_20,"['gitlab', '2025', 'utc', '20', '13', '11', 'com', '57', 'https', 'gitlab com']","['New year, new activities! Hello, GitLab Community! üëã\n\nCheck out the list of trends (and threats) in data protection! Enter 2025 safely!\n\n# üìö News & Resources\n\n**Blog Post üìù| GitLab 17.7 Release** GitLab 17.7 release introduces over 230 improvements. These include: a new Planner user role, auto-resolution policy for vulnerabilities, admin-controlled instance integration allowlists, access token rotation in the UI, and much more! GitLab expressed their gratitude towards the community for 138 contributions. üëâ [Find out more](https://about.gitlab.com/releases/2024/12/19/gitlab-17-7-released/)\xa0\n\n**Blog Post üìù| Data Protection And Backup Predictions For 2025 and beyond** Gartner predicts that by 2028, roughly 75% of organizations will be relying on SaaS applications for backup. Not a surprising statistic when we consider the rising cyber threats and more rigorous regulations. This article provides an overview of data protection trends predicted for 2025 and beyond! üëâ [Full article](https://gitprotect.io/blog/data-protection-and-backup-predictions-for-2025-and-beyond/)\n\n\xa0**Blog Post üìù| Automating with GitLab Duo, Part 3: Validating testing** This article outlines the tests that the author ran while trying to validate the impact of GitLab Duo on their team‚Äôs automated testing. The results gathered from this are discussed and show what has been achieved so far. üëâ [More details](https://about.gitlab.com/blog/2024/12/17/automating-with-gitlab-duo-part-3-validating-testing/)\xa0\n\n\xa0**Blog Post üìù| Transform code quality and compliance with automated processes** As you may know, manual code review may not be enough for DevSecOps-focused teams. GitLab outlines its premium features that address the technical debt and security vulnerability challenges of some of the traditional approaches. Learn more about compliance controls, review systems, and software security. üëâ [Read more](https://about.gitlab.com/blog/2024/12/13/transform-code-quality-and-compliance-with-automated-processes/)\xa0\xa0\n\n**Blog Post üìù| Best Practices for Securing Git LFS on GitHub, GitLab, Bitbucket, and Azure DevOps** As you may know, Git Large File Storage (LFS) is an open-source extension for Git, which can be used to handle versioning of larger files. It makes it easier for a developer to manage data since repositories are optimized - data is stored separately from the repo‚Äôs structure. It is also better to know how to protect this data well. üëâ [More details](https://gitprotect.io/blog/best-practices-for-securing-git-lfs-on-github-gitlab-bitbucket-and-azure-devops/)\n\n# üìÖ Upcoming Events\xa0\n\n**Webcast ü™ê | Transitioning from AWS CodeCommit to GitLab | Jan 23, 2025 | 3 pm GMT | Virtual** This webcast will cover topics from why organizations transition to GitLab, what benefits GitLab brings for DevSecOps and how to ensure a smooth transition. Since AWS CodeCommit has been deprecated, it‚Äôs good to guarantee a smooth migration, while keeping your development work, integrations, and processes secure. Check out expert opinions and best practices for a seamless migration! üëâ [Save your spot](https://page.gitlab.com/webcasts-january23-transitioning-aws-codecommit-to-gitlab.html)\n\n\xa0**Online Event ü™ê| GitLab Hackathon | Jan 23 - Jan 30, 2025 | Virtual** GitLab‚Äôs Hackathon is a great opportunity for devs interested in contributing code, translations, UX designs, and more to GitLab. Not only do you get to participate in things of interest to you, but you can actually improve your skills and knowledge over the 7-day hackathon! There will be prizes for participants and their merge requests. üëâ [Take part](https://about.gitlab.com/community/hackathon/)\n\n\xa0**Online Workshop ü™ê| AI in DevSecOps: Hands-on Workshop | Jan 30, 2025 | 2 pm - 5pm CET** This workshop will revolve around AI use in DevSecOps. Check out how a DevSecOps platform with AI can benefit you. It can improve your workflows, beyond code creation - actually streamline the entire software development lifecycle! üëâ [Secure your spot](https://page.gitlab.com/workshop_January30_DuoAIWorkshop_EMEA.html)\xa0\n\n‚úçÔ∏è *Subscribe to* [*GitProtect DevSecOps X-Ray Newsletter*](https://gitprotect.io/gitprotect-newsletter.html?utm_source=sm&utm_medium=ac) *and always stay tuned for more news!*', 'üéâThe **GitLab Hackathon** is now open!üöÄ  \nWe\'re excited to kick off another week of collaboration and innovation! Checkout our kickoff video [here](https://youtube.com/shorts/pAnjq3Xt7J4) and make sure to follow your progress on the hackathon [leaderboard](https://gitlab-community.gitlab.io/community-projects/merge-request-leaderboard/?&createdAfter=2025-01-23&createdBefore=2025-01-30&mergedBefore=2025-03-02&label=Hackathon).\n\n**Ready to contribute?**  \nContributions to all projects under the [gitlab-org](https://gitlab.com/gitlab-org) and [gitlab-com](https://gitlab.com/gitlab-com) groups qualify for the Hackathon. Additionally, contributions to [GitLab Components](https://gitlab.com/components) qualify.\n\n**Not sure what to work on**?\n\n* Checkout the #contribution\\_opportunities channel on Discord\n* Our teams have curated lists of issues ready for you to tackle:\n   * [gitlab-org/gitlab#510132](https://gitlab.com/gitlab-org/gitlab/-/issues/510132)(some issues qualify for bonuses points!)\n   * [gitlab-org/gitlab#513333](https://gitlab.com/gitlab-org/gitlab/-/issues/513333)\n\n**Need help**?  \nReach out to #contribute or ask for help from our [merge request coaches](https://docs.gitlab.com/ee/development/contributing/merge_request_coaches.html) using ""@gitlab-bot help"" in an issue or MR.\n\n**Want to know more?**  \nVisit the hackathon [page](https://about.gitlab.com/community/hackathon/).\n\n**Remember: MRs must be merged within 30 days to qualify.**', ""I try to create backups of my self-hosted GitLab (running in Docker container) with `sudo docker exec -t gitlab gitlab-backup create`. However, when i check `backups` the directory is still empty. Im grateful about any ideas what i'm missing!\n\n\n\nThis is the output:\n\n    2025-01-13 20:11:52 UTC -- Dumping database ...\n    2025-01-13 20:11:52 UTC -- Dumping PostgreSQL database gitlabhq_production ...\n    2025-01-13 20:11:56 UTC -- [DONE]\n    2025-01-13 20:11:56 UTC -- Dumping database ... done\n    2025-01-13 20:11:56 UTC -- Dumping repositories ...\n    ...\n    2025-01-13 20:11:57 UTC -- Dumping repositories ... done\n    2025-01-13 20:11:57 UTC -- Dumping uploads ...\n    2025-01-13 20:11:57 UTC -- Dumping uploads ... done\n    2025-01-13 20:11:57 UTC -- Dumping builds ...\n    2025-01-13 20:11:57 UTC -- Dumping builds ... done\n    2025-01-13 20:11:57 UTC -- Dumping artifacts ...\n    2025-01-13 20:11:57 UTC -- Dumping artifacts ... done\n    2025-01-13 20:11:57 UTC -- Dumping pages ...\n    2025-01-13 20:11:57 UTC -- Dumping pages ... done\n    2025-01-13 20:11:57 UTC -- Dumping lfs objects ...\n    2025-01-13 20:11:57 UTC -- Dumping lfs objects ... done\n    2025-01-13 20:11:57 UTC -- Dumping terraform states ...\n    2025-01-13 20:11:57 UTC -- Dumping terraform states ... done\n    2025-01-13 20:11:57 UTC -- Dumping container registry images ... [DISABLED]\n    2025-01-13 20:11:57 UTC -- Dumping packages ...\n    2025-01-13 20:11:57 UTC -- Dumping packages ... done\n    2025-01-13 20:11:57 UTC -- Dumping ci secure files ...\n    2025-01-13 20:11:57 UTC -- Dumping ci secure files ... done\n    2025-01-13 20:11:57 UTC -- Dumping external diffs ...\n    2025-01-13 20:11:57 UTC -- Dumping external diffs ... done\n    2025-01-13 20:11:57 UTC -- Creating backup archive: 1736799112_2025_01_13_17.7.1_gitlab_backup.tar ...\n    2025-01-13 20:11:57 UTC -- Creating backup archive: 1736799112_2025_01_13_17.7.1_gitlab_backup.tar ... done\n    2025-01-13 20:11:57 UTC -- Uploading backup archive to remote storage  ... [SKIPPED]\n    2025-01-13 20:11:57 UTC -- Deleting old backups ... [SKIPPED]\n    2025-01-13 20:11:57 UTC -- Deleting tar staging files ...\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/backup_information.yml\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/db\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/repositories\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/uploads.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/builds.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/artifacts.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/pages.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/lfs.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/terraform_state.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/packages.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/ci_secure_files.tar.gz\n    2025-01-13 20:11:57 UTC -- Cleaning up /var/opt/gitlab/backups/external_diffs.tar.gz\n    2025-01-13 20:11:57 UTC -- Deleting tar staging files ... done\n    2025-01-13 20:11:57 UTC -- Deleting backups/tmp ...\n    2025-01-13 20:11:57 UTC -- Deleting backups/tmp ... done\n    2025-01-13 20:11:57 UTC -- Warning: Your gitlab.rb and gitlab-secrets.json files contain sensitive data\n    and are not included in this backup. You will need these files to restore a backup.\n    Please back them up manually.\n    2025-01-13 20:11:57 UTC -- Backup 1736799112_2025_01_13_17.7.1 is done.\n    2025-01-13 20:11:57 UTC -- Deleting backup and restore PID file at [/opt/gitlab/embedded/service/gitlab-rails/tmp/backup_restore.pid] ... done""]"
2,1,38,1_kubernetes_https_app_com,"['kubernetes', 'https', 'app', 'com', 'helm', 'github', 'kubectl', 'using', 'service', 'github com']","[""**k8sgpt (sandbox)**\n\n[https://github.com/k8sgpt-ai/k8sgpt](https://github.com/k8sgpt-ai/k8sgpt) is a well-known one.\n\n**karpor (kusionstack subproject)**\n\n[https://github.com/KusionStack/karpor](https://github.com/KusionStack/karpor)\n\nIntelligence for Kubernetes. World's most promising Kubernetes Visualization Tool for Developer and Platform Engineering teams\n\n**kube-copilot (personal project from Azure)**\n\n[https://github.com/feiskyer/kube-copilot](https://github.com/feiskyer/kube-copilot) \n\n* Automate Kubernetes cluster operations using ChatGPT (GPT-4 or GPT-3.5).\n* Diagnose and analyze potential issues for Kubernetes workloads.\n* Generate Kubernetes manifests based on provided prompt instructions.\n* Utilize native\xa0`kubectl`\xa0and\xa0`trivy`\xa0commands for Kubernetes cluster access and security vulnerability scanning.\n* Access the web and perform Google searches without leaving the terminal.\n\n**some cost related \\`observibility and analysis\\`**\n\nI did not check if all below projects focus on k8s.\n\n\\- opencost\n\n\\- kubecost\n\n\\- karpenter\n\n\\- crane\n\n\\- infracost\n\nAre there any ai-for-k8s projects that I miss?"", 'If anyone has just started playing with Kubernetes, below project would help them to understand many key concepts around Kubernetes. I just deployed it yesterday and open for feedback on this.\n\nIn this Project , you are required to build a containerized application that consists of a Flask web application and a MySQL database. The two components will be deployed on a public cloud Kubernetes cluster in separate namespaces with proper configuration management using ConfigMaps and Secrets.\n\n# Prerequisite\n\n* Kubernetes Cluster (can be a local cluster like Minikube or a cloud-based one).\n* kubectl installed and configured to interact with your Kubernetes cluster.\n* Docker installed on your machine to build and push the Docker image of the Flask app.\n* Docker Hub account to push the Docker image.\n\n# Setup Architecture\n\nhttps://preview.redd.it/t1xwdp14xcke1.png?width=2288&format=png&auto=webp&s=6f746aceb41b49166efba56b76dcd3ac1bbf9bd5\n\nYou will practically use the following key Kubernetes objects. It will help you understand how these objects can be used in real-world project implementations:\n\n* Deployment\n* HPA\n* ConfigMap\n* Secrets\n* StatefulSet\n* Service\n* Namespace\n\n# Build the Python Flask Application\n\nCreate a [app.py](http://app.py) file with following content\n\n    from\xa0flask\xa0import\xa0Flask, jsonify\n    import\xa0os\n    import\xa0mysql.connector\n    from\xa0mysql.connector\xa0import\xa0Error\n    \n    app\xa0=\xa0Flask(__name__)\n    \n    def\xa0get_db_connection():\n        """"""\n    \xa0\xa0\xa0\xa0Establishes a connection to the MySQL database using environment variables.\n    \xa0\xa0\xa0\xa0Expected environment variables:\n    \xa0\xa0\xa0\xa0\xa0\xa0- MYSQL_HOST\n    \xa0\xa0\xa0\xa0\xa0\xa0- MYSQL_DB\n    \xa0\xa0\xa0\xa0\xa0\xa0- MYSQL_USER\n    \xa0\xa0\xa0\xa0\xa0\xa0- MYSQL_PASSWORD\n    \xa0\xa0\xa0\xa0""""""\n        host\xa0=\xa0os.environ.get(""MYSQL_HOST"", ""localhost"")\n        database\xa0=\xa0os.environ.get(""MYSQL_DB"", ""flaskdb"")\n        user\xa0=\xa0os.environ.get(""MYSQL_USER"", ""flaskuser"")\n        password\xa0=\xa0os.environ.get(""MYSQL_PASSWORD"", ""flaskpass"")\n        \n        try:\n            connection\xa0=\xa0mysql.connector.connect(\n                host=host,\n                database=database,\n                user=user,\n                password=password\n            )\n            if\xa0connection.is_connected():\n                return\xa0connection\n        except\xa0Error\xa0as\xa0e:\n            app.logger.error(f""Error connecting to MySQL: {e}"")\n        return\xa0None\n    \n    u/app.route(""/"")\n    def\xa0index():\n        return\xa0f""Welcome to the Flask App running in {os.environ.get(\'APP_ENV\', \'development\')}\xa0mode!""\n    \n    u/app.route(""/dbtest"")\n    def\xa0db_test():\n        """"""\n    \xa0\xa0\xa0\xa0A simple endpoint to test the MySQL connection.\n    \xa0\xa0\xa0\xa0Executes a query to get the current time from the database.\n    \xa0\xa0\xa0\xa0""""""\n        connection\xa0=\xa0get_db_connection()\n        if\xa0connection\xa0is\xa0None:\n            return\xa0jsonify({""error"": ""Failed to connect to MySQL database""}), 500\n        try:\n            cursor\xa0=\xa0connection.cursor()\n            cursor.execute(""SELECT NOW();"")\n            current_time\xa0=\xa0cursor.fetchone()\n            return\xa0jsonify({\n                ""message"": ""Successfully connected to MySQL!"",\n                ""current_time"": current_time[0]\n            })\n        except\xa0Error\xa0as\xa0e:\n            return\xa0jsonify({""error"": str(e)}), 500\n        finally:\n            if\xa0connection\xa0and\xa0connection.is_connected():\n                cursor.close()\n                connection.close()\n    \n    if\xa0__name__\xa0==\xa0""__main__"":\n        debug_mode\xa0=\xa0os.environ.get(""DEBUG"", ""false"").lower() ==\xa0""true""\n        app.run(host=""0.0.0.0"", port=5000, debug=debug_mode)\n\n# Create a Dockerfile for the app\n\n    FROM\xa0python:3.9-slim\n    \n    #\xa0Install ping (iputils-ping) for troubleshooting\n    RUN\xa0apt-get update && apt-get install -y iputils-ping && rm -rf /var/lib/apt/lists/*\n    \n    WORKDIR\xa0/app\n    COPY\xa0requirements.txt .\n    RUN\xa0pip install --upgrade pip && pip install --no-cache-dir -r requirements.txt\n    COPY\xa0app.py .\n    \n    EXPOSE\xa05000\n    ENV\xa0FLASK_APP=app.py\n    \n    CMD\xa0[""python"", ""app.py""]\n\n# Build and Push the docker Image\n\n    docker build -t becloudready/my-flask-app\n\n# Login to DockerHub\n\n    docker login\n\nIt will show a 6 digit Code, which you need to enter to following URL\n\n[https://login.docker.com/activate](https://login.docker.com/activate)\n\nPush the Image to DockerHub\n\n    docker push becloudready/my-flask-app\n\nYou should be able to see the Pushed Image\n\n# Flask Deployment (flask-deployment.yaml)\n\n    apiVersion: apps/v1\n    kind: Deployment\n    metadata:\n      name: flask-deployment\n      namespace: flask-app\n      labels:\n        app: flask\n    spec:\n      replicas: 2\n      selector:\n        matchLabels:\n          app: flask\n      template:\n        metadata:\n          labels:\n            app: flask\n        spec:\n          containers:\n          - name: flask\n            image: becloudready/my-flask-app:latest  #\xa0Replace with your Docker Hub image name.\n            ports:\n            - containerPort: 5000\n            env:\n            - name: APP_ENV\n              valueFrom:\n                configMapKeyRef:\n                  name: flask-config\n                  key: APP_ENV\n            - name: DEBUG\n              valueFrom:\n                configMapKeyRef:\n                  name: flask-config\n                  key: DEBUG\n            - name: MYSQL_DB\n              valueFrom:\n                configMapKeyRef:\n                  name: flask-config\n                  key: MYSQL_DB\n            - name: MYSQL_HOST\n              valueFrom:\n                configMapKeyRef:\n                  name: flask-config\n                  key: MYSQL_HOST\n            - name: MYSQL_USER\n              valueFrom:\n                secretKeyRef:\n                  name: db-credentials\n                  key: username\n            - name: MYSQL_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: db-credentials\n                  key: password\n\n# Flask Service (flask-svc.yaml)\n\n    apiVersion: v1\n    kind: Service\n    metadata:\n      name: flask-svc\n      namespace: flask-app\n    spec:\n      selector:\n        app: flask\n      type: LoadBalancer\n      ports:\n      - port: 80\n        targetPort: 5000\n\n# ConfigMap for Flask App (flask-config.yaml)\n\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: flask-config\n      namespace: flask-app\n    data:\n      APP_ENV: production\n      DEBUG: ""false""\n      MYSQL_DB: flaskdb\n      MYSQL_HOST: mysql-svc.mysql.svc.cluster.local\n\n# Namespaces (namespaces.yaml)\n\n    apiVersion: v1\n    kind: Namespace\n    metadata:\n      name: flask-app\n    ---\n    apiVersion: v1\n    kind: Namespace\n    metadata:\n      name: mysql\n\n# \n\n# Secret for DB Credentials (db-credentials.yaml)\n\n    kubectl create secret generic db-credentials \\\n      --namespace=flask-app \\\n      --from-literal=username=flaskuser \\\n      --from-literal=password=flaskpass \\\n      --from-literal=database=flaskdb\n\n# Setup and Configure MySQL Pods\n\n# ConfigMap for MySQL Init Script (mysql-initdb.yaml)\n\n    apiVersion: v1\n    kind: ConfigMap\n    metadata:\n      name: mysql-initdb\n      namespace: mysql\n    data:\n      initdb.sql: |\n    \xa0\xa0\xa0\xa0CREATE DATABASE IF NOT EXISTS flaskdb;\n    \xa0\xa0\xa0\xa0CREATE USER \'flaskuser\'@\'%\' IDENTIFIED BY \'flaskpass\';\n    \xa0\xa0\xa0\xa0GRANT ALL PRIVILEGES ON flaskdb.* TO \'flaskuser\'@\'%\';\n    \xa0\xa0\xa0\xa0FLUSH PRIVILEGES;\n\n# MySQL Service (mysql-svc.yaml)\n\n    apiVersion: v1\n    kind: Service\n    metadata:\n      name: mysql-svc\n      namespace: mysql\n    spec:\n      selector:\n        app: mysql\n      ports:\n      - port: 3306\n        targetPort: 3306\n\n# MySQL StatefulSet (mysql-statefulset.yaml)\n\n    apiVersion: apps/v1\n    kind: StatefulSet\n    metadata:\n      name: mysql-statefulset\n      namespace: mysql\n      labels:\n        app: mysql\n    spec:\n      serviceName: ""mysql-svc""\n      replicas: 1\n      selector:\n        matchLabels:\n          app: mysql\n      template:\n        metadata:\n          labels:\n            app: mysql\n        spec:\n          initContainers:\n          - name: init-clear-mysql-data\n            image: busybox\n            command: [""sh"", ""-c"", ""rm -rf /var/lib/mysql/*""]\n            volumeMounts:\n            - name: mysql-persistent-storage\n              mountPath: /var/lib/mysql\n          containers:\n          - name: mysql\n            image: mysql:5.7\n            ports:\n            - containerPort: 3306\n              name: mysql\n            env:\n            - name: MYSQL_ROOT_PASSWORD\n              value: rootpassword   #\xa0For production, use a Secret instead.\n            - name: MYSQL_DATABASE\n              value: flaskdb\n            - name: MYSQL_USER\n              value: flaskuser\n            - name: MYSQL_PASSWORD\n              value: flaskpass\n            volumeMounts:\n            - name: mysql-persistent-storage\n              mountPath: /var/lib/mysql\n            - name: initdb\n              mountPath: /docker-entrypoint-initdb.d\n          volumes:\n          - name: initdb\n            configMap:\n              name: mysql-initdb\n      volumeClaimTemplates:\n      - metadata:\n          name: mysql-persistent-storage\n        spec:\n          accessModes: [ ""ReadWriteOnce"" ]\n          resources:\n            requests:\n              storage: 1Gi\n          storageClassName: do-block-storage\n\n# Deploy to Kubernetes\n\n* Create Namespaces:\n\n\n\n    kubectl apply -f namespaces.yaml\n\n* Deploy ConfigMaps and Secrets:\n\n\n\n    kubectl apply -f flask-config.yaml \n    kubectl apply -f mysql-initdb.yaml \n    kubectl apply -f db-credentials.yaml\n\n* Deploy MySQL:\n\n\n\n    kubectl apply -f mysql-svc.yaml \n    kubectl apply -f mysql-statefulset.yaml\n\n* Deploy Flask App:\n\n\n\n    kubectl apply -f flask-deployment.yaml \n    kubectl apply -f flask-svc.yaml\n\nTest the Application\n\n\n\n`kubectl get svc -n flask-app`   \n`NAME        TYPE           CLUSTER-IP       EXTERNAL-IP      PORT(S)        AGE`   \n`flask-svc   LoadBalancer   10.109.112.171   146.190.190.51   80:32618/TCP   2m53s`  \n  \n  \n`curl` [`http://146.190.190.51/dbtest`](http://146.190.190.51/dbtest) `{""current_time"":""Wed, 19 Feb 2025 21:37:57 GMT"",""message"":""Successfully connected to MySQL!""}`\n\n# Troubleshooting\n\nUnable to connect to MySQL from Flask App\n\nLogin to the Flask app pod to ensure all values are loaded properly\n\n    kubectl exec -it flask-deployment-64c8955d64-hwz7m -n flask-app -- bash\n    \n    root@flask-deployment-64c8955d64-hwz7m:/app# env | grep -i mysql\n    MYSQL_DB=flaskdb\n    MYSQL_PASSWORD=flaskpass\n    MYSQL_USER=flaskuser\n    MYSQL_HOST=mysql-svc.mysql.svc.cluster.local\n\n# Testing\n\n* Flask App:Access the external IP provided by the LoadBalancer service to verify the app is running.\n* Database Connection:Use the /dbtest endpoint of the Flask app to confirm it connects to MySQL.\n* Troubleshooting:Use kubectl logs and kubectl exec to inspect pod logs and verify environment variables.', '## Why KubeVPN?\n\nIn the Kubernetes era, developers face a critical conflict between **cloud-native complexity** and **local development\nagility**. Traditional workflows force developers to:\n\n1. Suffer frequent `kubectl port-forward`/`exec` operations\n2. Set up mini Kubernetes clusters locally (e.g., minikube)\n3. Risk disrupting shared dev environments\n\nKubeVPN solves this through **cloud-native network tunneling**, seamlessly extending Kubernetes cluster networks to\nlocal machines with three breakthroughs:\n\n- üöÄ **Zero-Code Integration**: Access cluster services without code changes\n- üíª **Real-Environment Debugging**: Debug cloud services in local IDEs\n- üîÑ **Bidirectional Traffic Control**: Route specific traffic to local or cloud\n\n![KubeVPN Architecture](https://raw.githubusercontent.com/kubenetworks/kubevpn/master/samples/flat_log.png)\n\n## Core Capabilities\n\n### 1. Direct Cluster Networking\n\n```bash\nkubevpn connect\n```\n\nInstantly gain:\n\n- ‚úÖ Service name access (e.g., `productpage.default.svc`)\n- ‚úÖ Pod IP connectivity\n- ‚úÖ Native Kubernetes DNS resolution\n\n```shell\n‚ûú curl productpage:9080 # Direct cluster access\n<!DOCTYPE html>\n<html>...</html>\n```\n\n### 2. Smart Traffic Interception\n\nPrecision routing via header conditions:\n\n```bash\nkubevpn proxy deployment/productpage --headers user=dev-team\n```\n\n- Requests with `user=dev-team` ‚Üí Local service\n- Others ‚Üí Original cluster handling\n\n### 3. Multi-Cluster Mastery\n\nConnect two clusters simultaneously:\n\n```bash\nkubevpn connect -n dev --kubeconfig ~/.kube/cluster1  # Primary\nkubevpn connect -n prod --kubeconfig ~/.kube/cluster2 --lite # Secondary\n```\n\n### 4. Local Containerized Dev\n\nClone cloud pods to local Docker:\n\n```bash\nkubevpn dev deployment/authors --entrypoint sh\n```\n\nLaunched containers feature:\n\n- üåê Identical network namespace\n- üìÅ Exact volume mounts\n- ‚öôÔ∏è Matching environment variables\n\n## Technical Deep Dive\n\nKubeVPN\'s three-layer architecture:\n\n| Component           | Function                     | Core Tech                  |\n|---------------------|------------------------------|----------------------------|\n| **Traffic Manager** | Cluster-side interception    | MutatingWebhook + iptables |\n| **VPN Tunnel**      | Secure local-cluster channel | tun device + WireGuard     |\n| **Control Plane**   | Config/state sync            | gRPC streaming + CRDs      |\n\n```mermaid\ngraph TD\n    Local[Local Machine] -->|Encrypted Tunnel| Tunnel[VPN Gateway]\n    Tunnel -->|Service Discovery| K8sAPI[Kubernetes API]\n    Tunnel -->|Traffic Proxy| Pod[Workload Pods]\n    subgraph K8s Cluster\n        K8sAPI --> TrafficManager[Traffic Manager]\n        TrafficManager --> Pod\n    end\n```\n\n## Performance Benchmark\n\n100QPS load test results:\n\n| Scenario      | Latency | CPU Usage | Memory |\n|---------------|---------|-----------|--------|\n| Direct Access | 28ms    | 12%       | 256MB  |\n| KubeVPN Proxy | 33ms    | 15%       | 300MB  |\n| Telepresence  | 41ms    | 22%       | 420MB  |\n\nKubeVPN outperforms alternatives in overhead control.\n\n## Getting Started\n\n### Installation\n\n```bash\n# macOS/Linux\nbrew install kubevpn\n\n# Windows\nscoop install kubevpn\n\n# Via Krew\nkubectl krew install kubevpn/kubevpn\n```\n\n### Sample Workflow\n\n1. **Connect Cluster**\n\n```bash\nkubevpn connect --namespace dev\n```\n\n2. **Develop & Debug**\n\n```bash\n# Start local service\n./my-service &\n\n# Intercept debug traffic\nkubevpn proxy deployment/frontend --headers x-debug=true\n```\n\n3. **Validate**\n\n```bash\ncurl -H ""x-debug: true"" frontend.dev.svc/cluster-api\n```\n\n## Ecosystem\n\nKubeVPN\'s growing toolkit:\n\n- üîå **VS Code Extension**: Visual traffic management\n- üß© **CI/CD Pipelines**: Automated testing/deployment\n- üìä **Monitoring Dashboard**: Real-time network metrics\n\nJoin developer community:\n\n```bash\n# Contribute your first PR\ngit clone https://github.com/kubenetworks/kubevpn.git\nmake kubevpn\n```\n\n---\n\n> Project URL: [https://github.com/kubenetworks/kubevpn](https://github.com/kubenetworks/kubevpn)  \n> Documentation: [Complete Guide](https://github.com/kubenetworks/kubevpn/wiki)  \n> Support: Slack #kubevpn\n\nWith KubeVPN, developers finally enjoy cloud-native debugging while sipping coffee ‚òïÔ∏èüöÄ']"
3,2,24,2_devops_experience_job_learning,"['devops', 'experience', 'job', 'learning', 'learn', 'tools', 'azure', 'automation', 'ai', 'devops tools']","['I started off learning about DevOps soon after I got into self hosting and running my own homelab, fast forward a few years this has become my addiction. I work with VoIP currently and play around with Linux a bit for work but nothing with containers or DevOps tools, so i have just been learning with my homelab.\n\nAnyways, Im sick of VoIP and my current role, and would like to start applying for some Jr DevOps roles but am curious from the people who actually do this as a job if you would think I am prepared enough just based on my homelab.\n\nPersonally I think i need to get better with Ansible, Kubernetes, adding more things to Terraform/OpenTofu, and learning coding languages, this is what I am working on currently.\n\nAll of the config can be located here [https://git.mafyuh.dev/mafyuh/iac](https://git.mafyuh.dev/mafyuh/iac) or on Github here [https://github.com/Mafyuh/iac](https://github.com/Mafyuh/iac)\n\nPlease critique and let me know what you think, this is my first time ever posting in DevOps so dont really know what to expect but id love to hear it all, good or bad. Thank you', ""Hello everyone.\nI'm currently tagged as a DevOps Engineer having following experience: \nAzure Webapp and VMs, Azure DevOps.\nI'm having 4.2 YOE since I started my career in IT industry. \nI don't have any kind of experience in K8s or docker or monitoring or jenkins or any other tools.\n\nI want to know how much should I be afraid of this AI impact?\nShould I change my domain from devops to data engineer or anything else?\nWhich DevOps Zone is AI impact proof(so that our job won't affeft much)\n\nI'm really afraid and in panic mode right now as people are getting laid off and these CEOs and big companies are coming up new thing every week that AI will impact our job.\nPlease guys HELP ME!!"", 'Hi everyone,\n\nI‚Äôm a DevOps Engineer with 5+ years of experience in cloud infrastructure, automation, and security. I have hands-on expertise in:\n\n‚úÖ Cloud Platforms: Azure, AWS\n‚úÖ Infrastructure as Code (IaC): Terraform, Bicep, ARM Templates\n‚úÖ CI/CD Pipelines: Azure DevOps, GitHub Actions, Jenkins, FluxCD\n‚úÖ Containers & Orchestration: Docker, Kubernetes (AKS, EKS)\n‚úÖ Monitoring & Logging: Prometheus, Grafana, ELK Stack, Azure Monitor\n‚úÖ Security & Compliance: OWASP ZAP, SonarCloud, SAST, DAST\n‚úÖ Cloud Cost Optimization: FinOps\n‚úÖ Scripting & Automation: PowerShell, Bash, Python\n\nI‚Äôm the founder of Nimbus Compute, where I provide DevOps consulting, mentorship, and training. I break down complex DevOps concepts for beginners, using real-life examples and hands-on projects. I also post educational DevOps content on TikTok, helping newcomers navigate the field.\n\nI am currently seeking a DevOps role in the UK and open to opportunities anywhere in the country. I‚Äôm ready to start immediately and eager to contribute my expertise to a great team.\n\nIf you know of any opportunities or have any advice, please reach out! I appreciate any leads, referrals, or networking connections.\n\nThanks in advance!\n\nLocation: Sheffield, England\nAvailability: Immediate\nLinkedIn: http://linkedin.com/in/ifebuche-omeke-9181b7325\n']"
4,3,20,3_ci cd_cd_ci_pipeline,"['ci cd', 'cd', 'ci', 'pipeline', 'people', 'cd pipeline', 'tools', 'use', 'project', 'pipelines']","[""Hi, I wonder how ci/cd should work and what instruments i have to use. \n\nI'm making my backend part of pet project and use docker. So i want to setup ci/cd fot my project and automatically integrate my new code to docker container. I'm confusing with a ci/cd pipeline. What tools i should use and how my new code will delivery to existing one. \n\nCan someone explain me that or maybe send any kind of guids. (videos, text tutorials, etc) Thanks in advance for any help."", 'The article below explains the concepts of CI and CD as automating code merging, testing and the release process. It also lists and describes popular CI/CD tools on how these tools manage large codebases and ensure effective adoption within teams: [The 14 Best CI/CD Tools For DevOps](https://www.codium.ai/blog/best-ci-cd-tools-for-devops/)\n\nThe tools mentioned include Jenkins, GitLab, CircleCI, TravisCI, Bamboo, TeamCity, Azure Pipelines, AWS CodePipeline, GitHub Actions, ArgoCD, CodeShip, GoCD, Spinnaker, and Harness.', 'Hello r/cicd,\n\nI work for a section of a university, that helps researchers, well... research i guess.  \nWe store data, grant access, manage the infrastrukture and assist in the researchers projects.\n\nAs you can imagine, these task lead to projects we do ourself. One of these projects was now handed down to me and 3 others, trying to answere the question, ""How can Ci/Cd help us?"" We are about 120 people, working on seperate tasks as described above. While we are that many people. usually only 2 to 4 people are assigned to projects, be that with other researchers or internal tasks such us maintaining the out-of-office tool (2 people) or the infrastukture that hosts the data and grants access to researcher groups (4 people)\n\nSo you see, every project that would benefit from Ci/Cd is itself smaller and most of the time does not lead to grander picture but is completed and archived after the project ends. Usually the documentation is then put onto an internal wiki for later re-use.\n\nBack to the question, ""How can Ci/Cd help us?"":  \nTeam Ci/Cd has met 3 times now, trying to understand where people in our organisation are using Ci/Cd. We found some attempts and some half automated pipelines, but not the complete picture. We started to ask if the complete automated pipeline is even something we would want.  \nThis is not my question to you.\n\nWhen should you use Ci/Cd?  \nIs there a minimum size of project members that should be reached to use the complete Ci/Cd pipeline?  \nIs it not related to project members but project complexity?  \nDo you always try to automate everything or have had a project yourself where you started with Ci/Cd but at the end decided to leave in a manual check?\n\nPS: Now that i have written all of this, I also wonder, is it worth to change the way an organisation works to make Ci/Cd lucrative or is better to ""not change a running system""?\n\nKind regards']"
5,4,19,4_docker_just_use_terraform,"['docker', 'just', 'use', 'terraform', 'services', 'like', 'run', 'deploy', 'github', 'containers']","['A detailed video guide on on how install docker applications to quickly set up a secure home media stack using Windows 11, Windows Subsystem for Linux, Ubuntu, and Docker, for managing and streaming media collections with applications like Jellyfin and Plex. Using Docker, MediaStack containerises these media servers alongside \\*ARR applications (Radarr, Sonarr, Lidarr, etc.) for seamless media automation and management.\n\nAlso uses Windows Service Wrapper, to WSL, Ubuntu, Docker and containers continue to run after reboots, without having to log in a start the service manually - always up.\n\nVideo Guide: [https://youtu.be/N--e1O5SqPw](https://youtu.be/N--e1O5SqPw)  \n  \nTechnical Guide / Steps:   [https://pastes.io/mediastack-a-detailed-guide-on-windows-11-docker-with-wsl-and-ubuntu](https://pastes.io/mediastack-a-detailed-guide-on-windows-11-docker-with-wsl-and-ubuntu)', ""Hey,\n\nI would like to tighten up the security of my Docker containers. The issue is that I feel like most beginner guides just show you the most basic way to start up a container without having security in mind and the more security-oriented guides are aimed at advanced users so they skim over steps that are important for beginners.\n\nLet's take a couple of examples from the commonly mentioned [OWASP cheatsheet](https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html):\n\n* Rule #3 says that you should only grant the necessary capabilities but how do I know what capabilities each container needs?\n* Rule #6 tells you to use linux security module but there is no further info outside of links to the docs which are honestly not understandable to me as a beginner\n* Rule #11 telling you to run Docker in rootless mode and while it mentions potential downsides through Docker docs they are not exactly comprehensible for a less experienced person (or at least for me)\n\nI'm also missing potential implications of messing with these settings because tightening security can easily lead to e.g. permission errors in my opinion. I personally don't have an issue with doing my own research as well but I feel like each rule in the cheatsheet can take you down its own rabbit hole and this way it gets too overwhelming for someone who is just starting out and only wants to spin up a couple of containers.\n\nI've also seen Podman mentioned quite often (even the OWASP cheatsheet mentions it) as a secure alternative to Docker. I'd prefer to stay with Docker since most guides are Docker-oriented but when I see how complicated securing Docker is I'm thinking whether it wouldn't be easier to just switch.\n\nSo as the title states, I'd like to know whether there are any beginner-friendly guides for securing Docker containers according to best practices or whether I should switch to Podman which should be more secure out of the box.\n\nThanks!"", 'TLDR: What\'s the best practice for managing infra with custom Docker based images using Terraform?\n\nWe primarily use GCP and for a lot of simple services we use Cloud Run with GAR (Google Artifact Registry) to store the Docker images.\n\nTo manage the infra, we generally use Terraform and we use GitHub Actions to do CI & CD.\n\n\n\nDeployments to new environments comprise of the following steps:\n\n1) \\[Terraform\\] Create a new GAR repository that Docker can push to\n\n2) \\[Docker\\] Build and push the Docker Image on the newly created GAR and then\n\n3) \\[Terraform\\] Deploy the Cloud Run service which uses the GAR, along side any other infrastructure we might need.\n\nThis 3 step process is usually how our CD (GitHub Actions) is structured and how our ""local"" dev (i.e. personal dev projects) works, both usually running with [just](https://github.com/casey/just) as the command runner. \n\nTerraform needs to have a ""bootstrap"" environment which gets deployed in the first step, separate from the ""main"" one used in the third. Although, instead of using a separate bootstrap environment, you can also use -target to apply just the GAR but that has its own downsides imo (not a fan of partial apply, especially if bootstrap involves additional steps such as service account creation and IAM role assignment).\n\n  \nIt\'s possible to avoid having two Terraform apply steps by doing one of the following:\n\n\\- Deploy the Cloud Run services manually using the gcloud CLI - but then you cannot manage it well via Terraform which can be problematic for certain situations.\n\n\\- Perform the bootstrap separately (perhaps manual operations?) so normal work doesn\'t require it - but this sounds like a recipe for non reproducible infra - might make disaster recovery painful\n\n\\- Run the docker commands as part of some terraform operator (using either a null resource with local exec or perhaps an existing provider such as [kreuzwerker/terraform-provider-docker](https://github.com/kreuzwerker/terraform-provider-docker)), but this might be slow for repetitive work and might just not integrate that well with Terraform\n\n\n\nAny suggestions how we can do this better? For trivial services it\'s a lot of boilerplate stuff that needs to be written, and it just drains the fun out of it tbh. With some work I suppose it\'s possible to reuse some of the code, but we might put some unnecessary constrains and abstracting it right might take some work.\n\nIn a totally different world from my day job, my hobby NextJS apps are trivial to develop and a lot more fun. I can focus on the app code instead of all this samey stuff which adds 0 business value.']"
6,5,18,5_pipeline_gitlab_ci_job,"['pipeline', 'gitlab', 'ci', 'job', 'shared', 'plan', 'bash', 'file', 'repository', 'build']","['**Hi GitLab Community,**\n\nI\'m looking for advice on how to structure my GitLab CI/CD pipelines when sharing functionality across repositories. Here‚Äôs my use case:\n\n### The Use Case\nI have two repositories:  \n- **repository1**: A project-specific repository. There will be multiple Repositorys like this including functionality from the ""gitlab-shared"" Repository\n- **gitlab-shared**: A repository for shared CI/CD functionality.\n\nIn **Repository 1**, I include shared functionality from the **GitLab Shared Repository** using `include: project` in my `.gitlab-ci.yml`:\n\n```yaml\n# ""repository1"" including the ""gitlab-shared"" repository for shared bash functions\ninclude:\n  # Include the shared library for common CI/CD functions\n  - project: \'mygroup/gitlab-shared\'\n    ref: main\n    file: \n      - \'ci/common.yml\' # Includes shared functionality such as bash exports\n```\n\nThe `common.yml` in the **GitLab Shared Repository** defines a hidden job to set up bash functions:\n\n```yaml\n# Shared functionality inside ""gitlab-shared""\n.setup_utility_functions:\n  script:\n    - |\n      function some_function(){\n        echo ""does some bash stuff that is needed in many repositories""\n      }\n      function some_function2(){\n        echo ""also does some complicated stuff""\n      }\n```\n\nIn **Repository 1**, I make these shared bash functions available like this:\n\n```yaml\n# Using the shared setup function to export bash functions in ""repository1""\ndefault:\n  before_script:\n    - !reference [.setup_utility_functions, script]\n```\n\nThis works fine, but here\'s my **problem**:\n\n---\n\n### The Problem\nAll the bash code for the shared functions is written inline in `common.yml` in the **GitLab Shared Repository**. I‚Äôd much prefer to extract these bash functions into a dedicated bash file for better readability in my IDE.\n\nHowever, because `include: project` only includes `.yml` files, I cannot reference bash files from the shared repository. The hidden job `.setup_utility_functions` in **Repository 1** fails because the bash file is not accessible.\n\n---\n\n### My Question\nIs there a better way to structure this? Ideally, I\'d like to:  \n1. Write the bash functions in a bash file in the **GitLab Shared Repository**.  \n2. Call this bash file from the hidden job `.setup_utility_functions` in **Repository 1**.\n\nRight now, I‚Äôve stuck to simple bash scripts for their readability and simplicity, but the lack of support for including bash files across repositories has become a little ugly.\n\nAny advice or alternative approaches would be greatly appreciated!\n\nThanks in advance! üòä', 'Hi GitLab Community,  \n\nI‚Äôm currently trying to implement dynamic variables in GitLab CI/CD pipelines and wanted to ask if there‚Äôs an easier or more efficient way to handle this. Here‚Äôs the approach I‚Äôm using right now:  \n\n### Current Approach  \nAt the start of the pipeline, I have a `prepare_pipeline` job that calculates the dynamic variables and provides a `prepare.env` file. Example:  \n\n```yaml\nprepare_pipeline:\n  stage: prepare\n  before_script:\n    # This will execute bash code that exports functions to calculate dynamic variables\n    - !reference [.setup_utility_functions, script]\n  script:\n    # Use the exported function from before_script, e.g., ""get_project_name_testing""\n    - PROJECT_NAME=$(get_project_name_testing)\n    - echo ""PROJECT_NAME=$PROJECT_NAME"" >> prepare.env\n  artifacts:\n    reports:\n      dotenv: prepare.env\n```  \n\nThis works, but I‚Äôm not entirely happy with the approach.  \n\n---\n\n### Things I Don‚Äôt Like About This Approach  \n1. **Manual Echoing**:  \n   - Every time someone adds a new environment variable calculation, they must remember to `echo` it into the `.env` file.  \n   - If they forget or make a mistake, it can break the pipeline, and it‚Äôs not always intuitive for people who aren‚Äôt familiar with GitLab CI/CD.  \n\n2. **Extra Job Overhead**:  \n   - The `prepare_pipeline` job runs before the main pipeline stages, which requires setting up a Docker container (we use a Docker executor).  \n    This slows down the pipeline\n\n---\n\n### My Question  \nIs there a **best practice** for handling dynamic variables more efficiently or easily in GitLab CI/CD? I‚Äôm open to alternative approaches, tools, or strategies that reduce overhead and simplify the process for developers.  \n\nThanks in advance for any advice or ideas! üòä', 'We are trying to disable that Auto DevOps feature on some of our projects and it doesn\'t seem to take effect.  We followed the instructions in [https://docs.gitlab.com/ee/topics/autodevops/](https://docs.gitlab.com/ee/topics/autodevops/) by unchecking the Default to AutoDev Ops pipeline box found in the projects Settings>CI/CD>Auto DevOps section.  However the pipeline is still starting automatically on every commit.  Does the fact that a .gitlab-ci.yml file exists at the root of the repository override the setting?\n\nEDIT: Here is a summary of what we are tying to do\n\n* Use Gitlab\'s CI/CD pipeline for only manual starts with the Run pipeline button\n* Use pre-filled variables that we want displayed in the run pipeline form with scoped options.  We got this working.\n* We do not want the pipeline to auto start on commits.\n\nHere is what we tried so far\n\n* Unchecked the project CI/CD Auto DevOps setting\n   * Still builds on commit\n* Used a different template file name at the root\n   * We were prompted to set up a pipeline with the default .gitlab-ci.yml file\n   * We could not run any pipelines\n* Used a different template file and set it in the project CI/CD general pipeline settings\n   * It started auto building on commit again\n* Added a workflow if rule where the CI\\_PIPELINE\\_SOURCE is ""web"" then run\n   * This seems to work however if someone misses this item in the template then it will auto build again.\n\nIs there a way in GitLab CI/CD to use the pipeline but have Auto DevOps disabled by default?  If so at what level can it be done at (System, Group, Project, etc)?']"
7,6,9,6_commit_repository_git_folder,"['commit', 'repository', 'git', 'folder', 'new', 'history', 'want', 'reset', 'commits', 'change']","[""I created a new folder to get the folder system but now I somehow deleted it trying to commit from VScode, because I had opened the folder and it wasn't commiting to github, so I opened a new one and then deleted the one that wasn't commiting and it deleted everything but the README file when I commited that one.\n\nI also didn't have all the folders on GitHub idk why, so I was also trying to fix that\n\nI had been using the terminal before this. I don't wanna create a new folder and start from scratch, I want to learn how to fix problems like this. I've already googled and they all want me to create a new repo\n\nWhen I use\n\ngit add FOLDERNAME/\n\nit just tells me I have nothing to commit"", 'I\'m new to Git--I\'ve used GitHub to share code and fork projects but don\'t know really anything about how Git works ""under the hood""?\n\nSo let\'s say that I want to clone a previous release of a repository, that may be 5+ years old, because after that, the owner of that repository made a change that I don\'t want to have in my version. Just doing this, as I understand, requires forking the repository as a whole and then ""checking out"" a previous commit of the repository. Let\'s say I manage to do this--so now I have a repository that has my own ""snapshot"" of that other repository as it was back then, that I can then modify in a new direction.\n\nNow let\'s say I want to incorporate into my own fork some changes to the original repository that happened more recently, that are in a different part of the code that doesn\'t affect the files modified by the change I\'m avoiding by cloning the earlier version. So I want to do something like merging, but effectively ""write protect"" the part that I want to keep from the earlier version, i.e. reject the commit(s) that introduced the unwanted change, and I would also need to have some sort of warning issued, and an option to reject, if potential commits to OTHER files reference functions, classes, etc. that were introduced by the change I\'m omitting, and that therefore don\'t exist in my branch(\\*).\n\nDoes Git allow this kind of filtering when merging? If not, is there a tool available that\'s made for this use case? And as a more ""philosophical"" question, is the better way to go about this to clone a newer version, then manually ""roll back"" just the change I want to skip by copying the old versions of the relevant files in place of the new ones, and then search *from there* for references to the new versions and modify those to work with the old versions of the rolled-back files?\n\n(\\*)I\'m well aware that this is *necessary*, but not *sufficient*, to detect changes that can\'t be made without breaking the code. Being able to check with certainty that code will work not only would require compiler-level knowledge of the relevant languages, it\'s likely NP complete, and in any case is FAR above what could be expected from something like Git. Even recursively detecting commits with broken references TO broken references would likely be hard--I\'m only aiming for ""one level deep"" filtering based on string searching for direct references to code that was added/modified in the skipped change.', ""Hello,  \nI have a repository with lfs enabled that had its history entirely removed. Its now a single commit with a single file. This happened a few month ago, and I couldnt find anybody with a local clone. I'm trying to recover/fix the history.\n\nI have tried `git log --oneline --all --graph --decorate --reflog`\nwhich gives me the latest commit and all the history earlier than the lastest merge request, but there are several commit that should be here. i.e.\n\n> *latest commit  \n> Here commits are missing  \n> * latest merge request\n> *rest of the history\n\n\nI have found on gitlab a commit hash relatively recent (almost the latest before the history deletion) and it still have almost all the history I need. I have tried to reset or checkout this commit without success (with `git reset --soft commit_sha` and `git checkout -b branch commit_sha`)\n\nAny idea how to reset to this commit?""]"
